{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Likelihood and Cross-Entropy Loss\n",
    "\n",
    "## Softmax Function and Conditional Probabilities\n",
    "\n",
    "The softmax function gives us a vector $\\hat{\\mathbf{y}}$, which we interpret as the estimated conditional probabilities of each class given any input $\\mathbf{x}$:\n",
    "$$\n",
    "\\hat{y}_j = P(y = j \\mid \\mathbf{x})\n",
    "$$\n",
    "where $\\hat{y}_j$ is the predicted probability for class $j$. \n",
    "\n",
    "Suppose the entire dataset $\\mathcal{D}$ has $n$ examples, where the example indexed by $i$ consists of a feature vector $\\mathbf{x}_i$ and a one-hot label vector $\\mathbf{y}_i$. To compare the model's predictions with reality, we evaluate the likelihood of the actual classes under the model given the features:\n",
    "$$\n",
    "P(\\mathcal{D}) = \\prod_{i=1}^n P(\\mathbf{y}_i \\mid \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "\n",
    "According to maximum likelihood estimation, we maximize the log-likelihood:\n",
    "$$\n",
    "\\log P(\\mathcal{D}) = \\sum_{i=1}^n \\log P(\\mathbf{y}_i \\mid \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "Equivalently, this is equivalent to minimizing the negative log-likelihood:\n",
    "$$\n",
    "-\\log P(\\mathcal{D}) = -\\sum_{i=1}^n \\log P(\\mathbf{y}_i \\mid \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "## Cross-Entropy Loss\n",
    "\n",
    "For any pair of label $\\mathbf{y}$ and model prediction $\\hat{\\mathbf{y}}$ over $k$ classes, the loss function $\\mathcal{L}$ is defined as:\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{j=1}^k y_j \\log \\hat{y}_j\n",
    "$$\n",
    "\n",
    "This is commonly called the cross-entropy loss. Since $\\mathbf{y}$ is a one-hot vector, only the term corresponding to the actual class label contributes to the loss:\n",
    "$$\n",
    "\\mathcal{L} = - \\log \\hat{y}_{\\text{true}}\n",
    "$$\n",
    "\n",
    "## Properties of the Cross-Entropy Loss\n",
    "\n",
    "1. **Range of the Loss**:\n",
    "   - All $\\hat{y}_j$ are probabilities, so $0 \\leq \\hat{y}_j \\leq 1$.\n",
    "   - The logarithm of probabilities is always $\\leq 0$, and the loss is non-negative.\n",
    "\n",
    "2. **Minimization Condition**:\n",
    "   - The loss function is minimized when $\\hat{y}_{\\text{true}} = 1$, which corresponds to predicting the actual label with certainty.\n",
    "\n",
    "3. **Practical Challenges**:\n",
    "   - Label noise: Some labels in the dataset may be incorrect.\n",
    "   - Insufficient input information: Input features may not contain enough information to perfectly classify all examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Cross-Entropy Loss Calculation\n",
    "\n",
    "## Given\n",
    "\n",
    "Let the true label $\\mathbf{y}$ and predicted probabilities $\\hat{\\mathbf{y}}$ be:\n",
    "$$\n",
    "\\mathbf{y} = [1, 0, 0], \\quad \\hat{\\mathbf{y}} = [0.8, 0.1, 0.1]\n",
    "$$\n",
    "\n",
    "The cross-entropy loss is defined as:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^k y_j \\log \\hat{y}_j\n",
    "$$\n",
    "\n",
    "where $k$ is the number of classes.\n",
    "\n",
    "## Step-by-Step Calculation\n",
    "\n",
    "1. Substitute the values of $\\mathbf{y}$ and $\\hat{\\mathbf{y}}$:\n",
    "   $$ \n",
    "   \\mathcal{L}([1, 0, 0], [0.8, 0.1, 0.1]) = -\\left( y_1 \\log \\hat{y}_1 + y_2 \\log \\hat{y}_2 + y_3 \\log \\hat{y}_3 \\right)\n",
    "   $$\n",
    "\n",
    "2. Expand using the one-hot encoding $\\mathbf{y} = [1, 0, 0]$:\n",
    "   $$\n",
    "   \\mathcal{L} = -\\left( 1 \\cdot \\log(0.8) + 0 \\cdot \\log(0.1) + 0 \\cdot \\log(0.1) \\right)\n",
    "   $$\n",
    "\n",
    "3. Simplify:\n",
    "   $$\n",
    "   \\mathcal{L} = -\\log(0.8)\n",
    "   $$\n",
    "\n",
    "4. Compute the logarithm (in base $e$ or natural log):\n",
    "   $$\n",
    "   \\log(0.8) \\approx -0.22314\n",
    "   $$\n",
    "\n",
    "5. Final result:\n",
    "   $$\n",
    "   \\mathcal{L} \\approx 0.22314\n",
    "   $$\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "The loss value $\\mathcal{L} = 0.22314$ indicates how far the predicted probability $\\hat{\\mathbf{y}}$ is from the true label $\\mathbf{y}$. A lower loss corresponds to a prediction closer to the true label. (Note $log(1) = 0$, and $y_{pred}$ will never larger than 1 since its probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Cross-Entropy Loss with Matrices\n",
    "\n",
    "## Given\n",
    "\n",
    "Suppose we have a minibatch of two examples, with the following true labels $\\mathbf{Y}$ and predicted probabilities $\\hat{\\mathbf{Y}}$:\n",
    "\n",
    "### True Labels (One-hot encoded)\n",
    "$$\n",
    "\\mathbf{Y} = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The first example belongs to class 1.\n",
    "- The second example belongs to class 2.\n",
    "\n",
    "### Predicted Probabilities\n",
    "$$\n",
    "\\hat{\\mathbf{Y}} = \\begin{bmatrix}\n",
    "0.8 & 0.1 & 0.1 \\\\\n",
    "0.2 & 0.7 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- For the first example, the model predicts class 1 with probability 0.8, class 2 with probability 0.1, and class 3 with probability 0.1.\n",
    "- For the second example, the model predicts class 1 with probability 0.2, class 2 with probability 0.7, and class 3 with probability 0.1.\n",
    "\n",
    "## Cross-Entropy Loss for Each Example\n",
    "\n",
    "The cross-entropy loss for each example is computed as:\n",
    "$$\n",
    "\\mathcal{L}_i = - \\sum_{j=1}^k y_{ij} \\log \\hat{y}_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "where $y_{ij}$ is the true label (one-hot encoded) and $\\hat{y}_{ij}$ is the predicted probability for class $j$ for the $i$-th example.\n",
    "\n",
    "---\n",
    "For the First Example ($\\mathbf{y}_1 = [1, 0, 0]$ and $\\hat{\\mathbf{y}}_1 = [0.8, 0.1, 0.1]$):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_1 = -\\left( 1 \\cdot \\log(0.8) + 0 \\cdot \\log(0.1) + 0 \\cdot \\log(0.1) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_1 = -\\log(0.8) \\approx 0.22314\n",
    "$$\n",
    "\n",
    "For the Second Example ($\\mathbf{y}_2 = [0, 1, 0]$ and $\\hat{\\mathbf{y}}_2 = [0.2, 0.7, 0.1]$):\n",
    "$$\n",
    "\\mathcal{L}_2 = -\\left( 0 \\cdot \\log(0.2) + 1 \\cdot \\log(0.7) + 0 \\cdot \\log(0.1) \\right)\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_2 = -\\log(0.7) \\approx 0.35667\n",
    "$$\n",
    "\n",
    "## Total Loss for the Minibatch\n",
    "\n",
    "The total loss for the minibatch is the average of the losses for all examples:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_i\n",
    "$$\n",
    "where $n$ is the number of examples in the minibatch.\n",
    "\n",
    "In this case:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\frac{1}{2} \\left( 0.22314 + 0.35667 \\right) = 0.28991\n",
    "$$\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The cross-entropy loss for the first example is $0.22314$.\n",
    "- The cross-entropy loss for the second example is $0.35667$.\n",
    "- The total loss for the minibatch is $0.28991$.\n",
    "\n",
    "This process is applied to the entire minibatch, where each example contributes to the final loss based on the difference between the predicted probabilities and the true labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss, Softmax and Overfitting\n",
    "\n",
    "Label smoothing is a way of adding noise at the output targets, aka labels. Let’s assume that we have a classification problem. In most of them, we use a form of cross-entropy loss such as \n",
    "\n",
    "$$\n",
    "\\mathcal{L}_i = - \\sum_{j=1}^k y_{ij} \\log \\hat{y}_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "and softmax to output the final probabilities.\n",
    "\n",
    "\n",
    "The target vector has the form of $[0, 1 , 0 , 0]$. Because of the way softmax is formulated: \n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i / T}}{\\sum_{j} e^{z_j / T}}\n",
    "$$\n",
    "\n",
    "\n",
    "it can never achieve an output of 1 or 0. The best he can do is something like $[0.0003, 0.999, 0.0003, 0.0003]$. As a result, the model will continue to be trained, pushing the output values as high and as low as possible. The model will never converge. That, of course, will cause overfitting.\n",
    "\n",
    "To address that, label smoothing replaces the hard 0 and 1 targets by a small margin. Specifically, 0 are replaced with $\\frac{e}{k−1}$ and 1 with $1−e$ where k is the number of classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
