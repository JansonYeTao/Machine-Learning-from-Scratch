{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Function\n",
    "\n",
    "- https://kexue.fm/archives/4647"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而因为这样，它在两端的导数都趋于0 (两端的切线都是平的)，而因为我们是用梯度下降优化的，导数趋于零，使得每次更新的量都很少（正比于梯度），所以更新起来比较困难。尤其是层数多了之后，由于求导的链式法则，那么每次更新的量就正比于梯度的n\n",
    "次方，优化就更加困难了，因此刚开始的神经网络都做不深。\n",
    "\n",
    "一个标志性的激活函数就是ReLu函数，它的定义很简单：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
