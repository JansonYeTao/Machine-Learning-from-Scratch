{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Precision and Recall Intuitively: A Search System Example\n",
    "\n",
    "Understanding **precision** and **recall** is crucial for evaluating the effectiveness of systems like search engines. Let’s break them down intuitively using a search system example.\n",
    "\n",
    "## Imagine a Library Search System\n",
    "\n",
    "**Scenario:** You’re using a library’s online search to find books about “Renewable Energy.”\n",
    "\n",
    "1. **Total Relevant Books (All Good Matches):**\n",
    "   - Suppose there are **100** books in the library that are truly about renewable energy.\n",
    "\n",
    "2. **Search Results Returned by the System:**\n",
    "   - The system returns **20** books when you search for “Renewable Energy.”\n",
    "\n",
    "3. **Relevant vs. Irrelevant in Search Results:**\n",
    "   - Out of these 20, **15** are truly about renewable energy (relevant).\n",
    "   - The remaining **5** are not closely related (irrelevant).\n",
    "\n",
    "### Precision\n",
    "\n",
    "- **Definition:** Precision measures how many of the retrieved results are actually relevant.\n",
    "- **Formula:** \n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{Number of Relevant Results Returned}}{\\text{Total Number of Results Returned}}\n",
    "  $$\n",
    "- **Calculation for Our Example:**\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{15}{20} = 0.75 \\text{ or } 75\\%\n",
    "  $$\n",
    "- **Intuitive Meaning:** Out of all the books the system suggested, 75% were genuinely about renewable energy. High precision means fewer irrelevant results are shown.\n",
    "\n",
    "### Recall\n",
    "\n",
    "- **Definition:** Recall measures how many of the total relevant results the system successfully retrieved.\n",
    "- **Formula:** \n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{Number of Relevant Results Returned}}{\\text{Total Number of Relevant Results}}\n",
    "  $$\n",
    "- **Calculation for Our Example:**\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{15}{100} = 0.15 \\text{ or } 15\\%\n",
    "  $$\n",
    "- **Intuitive Meaning:** The system found 15% of all the books about renewable energy available in the library. Low recall indicates that many relevant books were missed.\n",
    "\n",
    "## Visualizing Precision and Recall\n",
    "\n",
    "Think of it like fishing:\n",
    "\n",
    "- **Precision** is about **catching only the fish you want**. If you catch 20 fish and 15 are the right kind, your precision is high.\n",
    "- **Recall** is about **catching as many of the desired fish as possible**. If there are 100 fish you want and you only catch 15, your recall is low.\n",
    "\n",
    "## Balancing Precision and Recall\n",
    "\n",
    "Often, improving one can lead to a decrease in the other:\n",
    "\n",
    "- **High Precision, Low Recall:** The system shows fewer results, but they’re mostly relevant. (e.g., returning only the top 10 most relevant books out of 100.)\n",
    "- **High Recall, Low Precision:** The system shows many results, capturing most relevant ones, but also includes many irrelevant ones. (e.g., returning 90 books, where 15 are relevant.)\n",
    "\n",
    "## Why Both Matter\n",
    "\n",
    "- **User Intent:** If a user prefers fewer, highly relevant results, precision is more important.\n",
    "- **Comprehensive Search Needs:** If a user wants to see as many relevant results as possible, even at the expense of some irrelevant ones, recall is more important.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Precision:** How accurate the search results are ($\\text{relevant results vs. total results returned}$).\n",
    "- **Recall:** How comprehensive the search results are ($\\text{relevant results returned vs. total relevant results available}$).\n",
    "\n",
    "By balancing precision and recall, a search system can be optimized to meet different user needs, ensuring both relevance and comprehensiveness in its results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Concepts\n",
    "\n",
    "Before diving into the relationships, it’s essential to understand the four fundamental components used to evaluate search systems (and many other classification systems):\n",
    "\n",
    "1. **True Positives (TP):**\n",
    "   - **Definition:** Relevant results that the system correctly retrieves.\n",
    "   - **Example:** Books about renewable energy that are correctly returned by the search.\n",
    "\n",
    "2. **False Positives (FP):**\n",
    "   - **Definition:** Irrelevant results that the system incorrectly retrieves.\n",
    "   - **Example:** Books not related to renewable energy but still returned in the search results.\n",
    "\n",
    "3. **False Negatives (FN):**\n",
    "   - **Definition:** Relevant results that the system fails to retrieve.\n",
    "   - **Example:** Books about renewable energy that exist in the library but are not shown in the search results.\n",
    "\n",
    "4. **True Negatives (TN):**\n",
    "   - **Definition:** Irrelevant results that the system correctly does not retrieve.\n",
    "   - **Example:** Books not related to renewable energy that are appropriately excluded from the search results.\n",
    "\n",
    "> **Note:** In many search systems, especially those dealing with vast numbers of irrelevant items, **True Negatives** are often not the primary focus because the number of irrelevant items is typically too large to manage or quantify effectively.\n",
    "\n",
    "## Relating Precision and Recall to TP, FP, and FN\n",
    "\n",
    "### Precision\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "  $$\n",
    "  \n",
    "- **Explanation:**\n",
    "  Precision measures the **accuracy** of the retrieved results. It answers the question: *Of all the results the system returned, how many were actually relevant?*\n",
    "\n",
    "- **Using Our Example:**\n",
    "  - **TP:** 15 (relevant books retrieved)\n",
    "  - **FP:** 5 (irrelevant books retrieved)\n",
    "  \n",
    "  $$\n",
    "  \\text{Precision} = \\frac{15}{15 + 5} = \\frac{15}{20} = 0.75 \\text{ or } 75\\%\n",
    "  $$\n",
    "\n",
    "### Recall\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "  $$\n",
    "  \n",
    "- **Explanation:**\n",
    "  Recall measures the **completeness** of the retrieval. It answers the question: *Of all the relevant results available, how many did the system actually retrieve?*\n",
    "\n",
    "- **Using Our Example:**\n",
    "  - **TP:** 15 (relevant books retrieved)\n",
    "  - **FN:** 85 (relevant books not retrieved, since total relevant books are 100)\n",
    "  \n",
    "  $$\n",
    "  \\text{Recall} = \\frac{15}{15 + 85} = \\frac{15}{100} = 0.15 \\text{ or } 15\\%\n",
    "  $$\n",
    "\n",
    "### Visualizing the Relationships\n",
    "\n",
    "Here’s a table to visualize how these components interact:\n",
    "\n",
    "|                     | **Relevant (Positive)** | **Irrelevant (Negative)** | **Total**     |\n",
    "|---------------------|-------------------------|---------------------------|---------------|\n",
    "| **Retrieved**       | True Positives (TP) = 15| False Positives (FP) = 5  | 20            |\n",
    "| **Not Retrieved**   | False Negatives (FN) = 85| True Negatives (TN) ≈ N/A | 85 + N/A      |\n",
    "| **Total**           | 100                     | Large Number              | 100 + Large   |\n",
    "\n",
    "> **Note:** The exact number of True Negatives (TN) isn't typically calculated in search systems due to the vast number of irrelevant items.\n",
    "\n",
    "## Understanding False Positives and False Negatives\n",
    "\n",
    "### False Positives (FP)\n",
    "\n",
    "- **Impact on Precision:**\n",
    "  - **High FP:** Lowers precision because more irrelevant items are included in the results.\n",
    "  - **Example:** If the system retrieves many books that aren't about renewable energy, users might find the results less trustworthy or useful.\n",
    "\n",
    "- **Management:**\n",
    "  - **Improving Precision:** Refine search algorithms to better distinguish between relevant and irrelevant items, use more specific keywords, or implement better filtering mechanisms.\n",
    "\n",
    "### False Negatives (FN)\n",
    "\n",
    "- **Impact on Recall:**\n",
    "  - **High FN:** Lowers recall because many relevant items are missed.\n",
    "  - **Example:** If many books about renewable energy are not shown, users may not find all the information they need.\n",
    "\n",
    "- **Management:**\n",
    "  - **Improving Recall:** Broaden search criteria, include synonyms or related terms, and ensure the search index is comprehensive.\n",
    "\n",
    "## Balancing Precision and Recall\n",
    "\n",
    "There’s often a trade-off between precision and recall:\n",
    "\n",
    "- **High Precision, Low Recall:**\n",
    "  - **Scenario:** The system returns very few results, most of which are relevant.\n",
    "  - **Pros:** Users see highly relevant results.\n",
    "  - **Cons:** Many relevant items are missed.\n",
    "\n",
    "- **High Recall, Low Precision:**\n",
    "  - **Scenario:** The system returns many results, capturing most relevant items but including many irrelevant ones.\n",
    "  - **Pros:** Users are likely to find most of what they’re looking for.\n",
    "  - **Cons:** Users may have to sift through more irrelevant results.\n",
    "\n",
    "### Example Adjustments\n",
    "\n",
    "- **To Increase Precision:**\n",
    "  - Use more specific search terms.\n",
    "  - Implement advanced filtering options.\n",
    "  - Improve the relevance ranking algorithms.\n",
    "\n",
    "- **To Increase Recall:**\n",
    "  - Use broader search terms or include synonyms.\n",
    "  - Expand the search index to cover more sources.\n",
    "  - Reduce overly strict filtering that might exclude relevant items.\n",
    "\n",
    "## Introducing F1-Score\n",
    "\n",
    "To balance precision and recall, the **F1-Score** is often used:\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "\n",
    "- **Explanation:**\n",
    "  The F1-Score provides a single metric that balances both precision and recall, especially useful when you need to find an optimal balance between the two.\n",
    "\n",
    "- **Using Our Example:**\n",
    "  $$\n",
    "  F1 = 2 \\times \\frac{0.75 \\times 0.15}{0.75 + 0.15} = 2 \\times \\frac{0.1125}{0.9} \\approx 0.25 \\text{ or } 25\\%\n",
    "  $$\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Precision** and **Recall** are critical metrics for evaluating search systems.\n",
    "- They are directly related to:\n",
    "  - **True Positives (TP):** Relevant results correctly retrieved.\n",
    "  - **False Positives (FP):** Irrelevant results incorrectly retrieved.\n",
    "  - **False Negatives (FN):** Relevant results not retrieved.\n",
    "- **Precision** focuses on the **accuracy** of the results, minimizing **FP**.\n",
    "- **Recall** focuses on the **completeness** of the results, minimizing **FN**.\n",
    "- Balancing precision and recall is essential, often using metrics like the **F1-Score** to achieve an optimal balance based on user needs.\n",
    "\n",
    "By understanding and managing **TP**, **FP**, and **FN**, you can effectively tune your search system to meet desired precision and recall levels, ensuring users have a satisfactory search experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
