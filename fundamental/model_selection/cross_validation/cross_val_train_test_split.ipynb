{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Train & Test Split can go wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> In practice, a random split like we've used here might not be a good idea -- here's what Dr Rachel Thomas has to say about it: \"One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a train_test_split method, this method takes a random subset of the data, which is a poor choice for many real-world problems.\"\n",
    "-> https://www.fast.ai/posts/2017-11-13-validation-sets.html\n",
    "\n",
    "Why? because Random splitting into train and validation data: -> -> https://www.fast.ai/posts/2017-11-13-validation-sets.html\n",
    "1. Might not be useful in some situation eg. time series prediction task.\n",
    "2. Lead to Data leakage, and your model ends up doing very well on validation set instead of test data.\n",
    "\n",
    "- https://www.alfredo.motta.name/cross-validation-done-wrong/\n",
    "- Data Leakage when oversampling and train & test split:\n",
    "  - https://beckernick.github.io/oversampling-modeling/\n",
    "  \n",
    "\n",
    "\n",
    "For the Target Encoding, it is the same thing as above article; we should always train-test split at first and then do the target encoding so that we don't bleed in data leakage info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
