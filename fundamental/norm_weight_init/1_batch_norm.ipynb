{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "Batch normalization (often called *BatchNorm*) is a technique used in deep neural networks to stabilize and accelerate the training process. It was introduced by Sergey Ioffe and Christian Szegedy in 2015 to address issues such as *internal covariate shift* and *vanishing/exploding gradients*.\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "1. Normalization Per Mini-Batch  \n",
    "   During training, for each mini-batch (of size $m$), BatchNorm normalizes each feature channel by subtracting the mini-batch mean and dividing by the mini-batch standard deviation:\n",
    "\n",
    "   $$\n",
    "   \\hat{x} = \\frac{x - \\mu_{\\text{batch}}}{\\sigma_{\\text{batch}} + \\epsilon}\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $x$ is the input value (e.g., activation from the previous layer)\n",
    "   - $\\mu_{\\text{batch}}$ is the mean of the mini-batch\n",
    "   - $\\sigma_{\\text{batch}}$ is the standard deviation of the mini-batch\n",
    "   - $\\epsilon$ is a small constant to avoid division by zero\n",
    "\n",
    "2. Trainable Scale and Shift  \n",
    "   After normalization, BatchNorm applies a trainable linear transformation:\n",
    "\n",
    "   $$\n",
    "   y = \\gamma \\hat{x} + \\beta\n",
    "   $$\n",
    "\n",
    "   where $\\gamma$ and $\\beta$ are learnable parameters that allow the network to “undo” any normalization if needed, effectively letting the model retain representational power.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. At Inference Time  \n",
    "   Instead of using the mini-batch mean and variance (which won’t be reliable for a single sample), BatchNorm uses an exponential moving average of the mean and variance collected during training. This ensures consistent normalization at inference.\n",
    "\n",
    "## Benefits\n",
    "\n",
    "1. Improved Training Stability  \n",
    "   By normalizing the inputs to each layer, BatchNorm helps reduce the risk of vanishing or exploding gradients. This makes deeper networks more trainable.\n",
    "\n",
    "2. Faster Training  \n",
    "   BatchNorm allows you to use higher learning rates because it stabilizes the distribution of intermediate activations. This often results in faster convergence.\n",
    "\n",
    "3. Regularization Effect  \n",
    "   The mini-batch statistics introduce a small amount of noise (because each mini-batch’s mean and variance differ slightly). This acts like a regularizer and can help reduce overfitting.\n",
    "\n",
    "4. Possible Removal of Dropout  \n",
    "   In some architectures, especially convolutional neural networks, BatchNorm can reduce the need for dropout, simplifying the model without harming performance.\n",
    "\n",
    "## Where It’s Used\n",
    "\n",
    "BatchNorm is commonly applied after a convolution or fully connected layer, and before applying the nonlinearity (e.g., ReLU). It has been widely adopted in state-of-the-art models for image classification, object detection, and many other tasks.\n",
    "\n",
    "## Practical Tips\n",
    "\n",
    "- **Batch Size**  \n",
    "  For BatchNorm to work effectively, you typically need a sufficiently large batch size (e.g., 32 or more). For very small batch sizes, the mini-batch statistics can become too noisy.\n",
    "\n",
    "- **Layer Order**  \n",
    "  A common pattern is:\n",
    "\n",
    "  $$\n",
    "  \\text{Convolution} \\rightarrow \\text{BatchNorm} \\rightarrow \\text{ReLU} \\rightarrow \\text{Pooling}\n",
    "  $$\n",
    "\n",
    "  In some cases, the order might differ based on the particular architecture or research best practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Example with Multivariate Data\n",
    "\n",
    "Below is a more detailed example illustrating how Batch Normalization works with **two features** instead of just one. We’ll go through both the training and testing stages.\n",
    "\n",
    "## Training Time\n",
    "\n",
    "### Step 1: Mini-Batch Data\n",
    "\n",
    "Assume we have two training examples, each with two features $(x_1, x_2)$:\n",
    "\n",
    "- Example 1: $[4, 10]$\n",
    "- Example 2: $[6, 12]$\n",
    "\n",
    "We have:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x^{(1)} &= [4, 10], \\\\\n",
    "x^{(2)} &= [6, 12].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Step 2: Compute Mini-Batch Mean and Variance\n",
    "\n",
    "We compute the mean and variance *separately* for each feature across the batch.\n",
    "\n",
    "1. **Mean for each feature**  \n",
    "\n",
    "   $$\n",
    "   \\mu_1 = \\frac{4 + 6}{2} = 5, \n",
    "   \\quad\n",
    "   \\mu_2 = \\frac{10 + 12}{2} = 11\n",
    "   $$\n",
    "\n",
    "   So the mini-batch mean vector is:\n",
    "   $$\n",
    "   \\mu_{\\text{batch}} = [5, \\, 11].\n",
    "   $$\n",
    "\n",
    "2. **Variance for each feature**  \n",
    "\n",
    "   For the first feature:\n",
    "   $$\n",
    "   \\sigma_1^2 \n",
    "   = \\frac{(4 - 5)^2 + (6 - 5)^2}{2}\n",
    "   = \\frac{1 + 1}{2}\n",
    "   = 1\n",
    "   $$\n",
    "\n",
    "   For the second feature:\n",
    "   $$\n",
    "   \\sigma_2^2 \n",
    "   = \\frac{(10 - 11)^2 + (12 - 11)^2}{2}\n",
    "   = \\frac{1 + 1}{2}\n",
    "   = 1\n",
    "   $$\n",
    "\n",
    "   So the mini-batch variance vector is:\n",
    "   $$\n",
    "   \\sigma_{\\text{batch}}^2 = [1, \\, 1].\n",
    "   $$\n",
    "\n",
    "3. **Standard Deviation**  \n",
    "\n",
    "   $$\n",
    "   \\sigma_{\\text{batch}} \n",
    "   = \\sqrt{ \\sigma_{\\text{batch}}^2 }\n",
    "   = [1, \\, 1].\n",
    "   $$\n",
    "\n",
    "### Step 3: Normalize the Inputs\n",
    "\n",
    "We normalize each feature of each example using the batch mean and standard deviation. If $\\epsilon$ is a small constant like $10^{-5}$:\n",
    "\n",
    "$$\n",
    "\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_{\\text{batch}}}{\\sigma_{\\text{batch}} + \\epsilon}.\n",
    "$$\n",
    "\n",
    "- For $x^{(1)} = [4, \\, 10]$:\n",
    "\n",
    "  $$\n",
    "  \\hat{x}^{(1)}_1 \n",
    "  = \\frac{4 - 5}{1 + 10^{-5}} \n",
    "  \\approx -0.99999, \n",
    "  \\quad\n",
    "  \\hat{x}^{(1)}_2 \n",
    "  = \\frac{10 - 11}{1 + 10^{-5}}\n",
    "  \\approx -0.99999.\n",
    "  $$\n",
    "\n",
    "  So:\n",
    "  $$\n",
    "  \\hat{x}^{(1)} \\approx [-0.99999, \\, -0.99999].\n",
    "  $$\n",
    "\n",
    "- For $x^{(2)} = [6, \\, 12]$:\n",
    "\n",
    "  $$\n",
    "  \\hat{x}^{(2)}_1 \n",
    "  = \\frac{6 - 5}{1 + 10^{-5}} \n",
    "  \\approx 0.99999, \n",
    "  \\quad\n",
    "  \\hat{x}^{(2)}_2 \n",
    "  = \\frac{12 - 11}{1 + 10^{-5}}\n",
    "  \\approx 0.99999.\n",
    "  $$\n",
    "\n",
    "  So:\n",
    "  $$\n",
    "  \\hat{x}^{(2)} \\approx [0.99999, \\, 0.99999].\n",
    "  $$\n",
    "\n",
    "### Step 4: Apply Scale and Shift\n",
    "\n",
    "Let the learnable scale parameter be $\\gamma = [2, \\, 2]$ and the learnable shift parameter be $\\beta = [3, \\, 5]$. We apply:\n",
    "\n",
    "$$\n",
    "y^{(i)} = \\gamma \\odot \\hat{x}^{(i)} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "- For $\\hat{x}^{(1)}$:\n",
    "\n",
    "  $$\n",
    "  y^{(1)} \n",
    "  = [2, 2] \\odot [-0.99999, -0.99999] + [3, 5]\n",
    "  \\approx [-1.99998, -1.99998] + [3, 5]\n",
    "  \\approx [1.00002, 3.00002].\n",
    "  $$\n",
    "\n",
    "- For $\\hat{x}^{(2)}$:\n",
    "\n",
    "  $$\n",
    "  y^{(2)} \n",
    "  = [2, 2] \\odot [0.99999, 0.99999] + [3, 5]\n",
    "  \\approx [1.99998, 1.99998] + [3, 5]\n",
    "  \\approx [4.99998, 6.99998].\n",
    "  $$\n",
    "\n",
    "Hence, the final outputs of the BatchNorm layer for this mini-batch are approximately:\n",
    "\n",
    "$$\n",
    "y^{(1)} \\approx [1.00002, \\, 3.00002], \n",
    "\\quad\n",
    "y^{(2)} \\approx [4.99998, \\, 6.99998].\n",
    "$$\n",
    "\n",
    "### Step 5: Update Running Mean and Variance\n",
    "\n",
    "We maintain a running average of the batch means and variances for each feature:\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{run}} \\leftarrow \\alpha \\mu_{\\text{run}} + (1 - \\alpha)\\mu_{\\text{batch}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{run}}^2 \\leftarrow \\alpha \\sigma_{\\text{run}}^2 + (1 - \\alpha)\\sigma_{\\text{batch}}^2\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a momentum term (e.g., 0.9).\n",
    "\n",
    "## Testing Time\n",
    "\n",
    "During testing (inference), we typically have a single example or a different batch size. We **do not** compute mean and variance from the test examples. Instead, we use the *stored* (running) statistics from training: $\\mu_{\\text{run}}$ and $\\sigma_{\\text{run}}^2$.\n",
    "\n",
    "For a test example $x_{\\text{test}}$ with 2 features, the normalized output is:\n",
    "\n",
    "$$\n",
    "\\hat{x}_{\\text{test}} \n",
    "= \\frac{x_{\\text{test}} - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Then we apply the learned scale and shift:\n",
    "\n",
    "$$\n",
    "y_{\\text{test}} = \\gamma \\odot \\hat{x}_{\\text{test}} + \\beta.\n",
    "$$\n",
    "\n",
    "Using these running statistics ensures the model behaves consistently, regardless of test-time batch sizes or single-sample inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Statistics Example with Two Features\n",
    "\n",
    "During each training iteration:\n",
    "1. Compute the mini-batch mean and variance.\n",
    "2. Update the running mean and variance using a momentum or smoothing factor.\n",
    "3. Use the mini-batch mean/variance for that training step’s normalization.\n",
    "\n",
    "At test time, you simply use the *final* stored running mean and variance (accumulated throughout training), rather than recomputing from the test samples.\n",
    "\n",
    "Below, we’ll illustrate how BatchNorm’s running mean and variance are updated **across multiple training iterations** when the same mini-batch data is encountered each time. This corresponds to the **two-feature** example:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x^{(1)} &= [4, \\, 10],\\\\\n",
    "x^{(2)} &= [6, \\, 12].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Mini-Batch Statistics\n",
    "\n",
    "For each training iteration, we assume the **same** mini-batch:\n",
    "- Two samples, each with 2 features:\n",
    "  - $x^{(1)} = [4, \\, 10]$\n",
    "  - $x^{(2)} = [6, \\, 12]$\n",
    "\n",
    "1. **Batch Mean**  \n",
    "   For feature 1 (the first component of each vector):\n",
    "   $$\n",
    "   \\mu_1 = \\frac{4 + 6}{2} = 5.\n",
    "   $$\n",
    "   For feature 2 (the second component of each vector):\n",
    "   $$\n",
    "   \\mu_2 = \\frac{10 + 12}{2} = 11.\n",
    "   $$\n",
    "   So:\n",
    "   $$\n",
    "   \\mu_{\\text{batch}} = [5, \\, 11].\n",
    "   $$\n",
    "\n",
    "2. **Batch Variance**  \n",
    "   For feature 1:\n",
    "   $$\n",
    "   \\sigma_1^2 \n",
    "   = \\frac{(4 - 5)^2 + (6 - 5)^2}{2}\n",
    "   = \\frac{1 + 1}{2}\n",
    "   = 1.\n",
    "   $$\n",
    "   For feature 2:\n",
    "   $$\n",
    "   \\sigma_2^2 \n",
    "   = \\frac{(10 - 11)^2 + (12 - 11)^2}{2}\n",
    "   = \\frac{1 + 1}{2}\n",
    "   = 1.\n",
    "   $$\n",
    "   Hence:\n",
    "   $$\n",
    "   \\sigma_{\\text{batch}}^2 = [1, \\, 1].\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Initialization and Momentum\n",
    "\n",
    "We’ll track the **running mean** ($\\mu_{\\text{run}}$) and **running variance** ($\\sigma_{\\text{run}}^2$) over multiple iterations. Suppose:\n",
    "\n",
    "- **Initial** running mean: $\\mu_{\\text{run}}^{(0)} = [0, \\, 0]$\n",
    "- **Initial** running variance: $\\sigma_{\\text{run}}^{2,(0)} = [1, \\, 1]$\n",
    "- **Momentum** (or smoothing factor): $\\alpha = 0.9$\n",
    "\n",
    "After each iteration, we update:\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{run}}^{(t+1)}\n",
    "\\;\\leftarrow\\;\n",
    "\\alpha\\, \\mu_{\\text{run}}^{(t)}\n",
    "\\;+\\;(1 - \\alpha)\\,\\mu_{\\text{batch}},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{run}}^{2,(t+1)}\n",
    "\\;\\leftarrow\\;\n",
    "\\alpha\\, \\sigma_{\\text{run}}^{2,(t)}\n",
    "\\;+\\;(1 - \\alpha)\\,\\sigma_{\\text{batch}}^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Batch #1\n",
    "\n",
    "- **Given:**  \n",
    "  - $\\mu_{\\text{run}}^{(0)} = [0, \\, 0]$  \n",
    "  - $\\sigma_{\\text{run}}^{2,(0)} = [1, \\, 1]$  \n",
    "  - $\\mu_{\\text{batch}} = [5, \\, 11]$  \n",
    "  - $\\sigma_{\\text{batch}}^2 = [1, \\, 1]$\n",
    "\n",
    "1. **Update Running Mean**  \n",
    "   $$\n",
    "   \\mu_{\\text{run}}^{(1)}\n",
    "   = \\alpha \\, [0,\\,0] \\;+\\; (1 - \\alpha)\\,[5,\\,11]\n",
    "   = 0.9 \\,[0,\\,0] \\;+\\; 0.1 \\,[5,\\,11].\n",
    "   $$\n",
    "   Therefore:\n",
    "   $$\n",
    "   \\mu_{\\text{run}}^{(1)} = [0.5,\\; 1.1].\n",
    "   $$\n",
    "\n",
    "2. **Update Running Variance**  \n",
    "   $$\n",
    "   \\sigma_{\\text{run}}^{2,(1)}\n",
    "   = 0.9 \\,[1,\\,1]\n",
    "   \\;+\\; 0.1 \\,[1,\\,1]\n",
    "   = [0.9,\\,0.9] + [0.1,\\,0.1]\n",
    "   = [1,\\,1].\n",
    "   $$\n",
    "\n",
    "So after iteration 1:\n",
    "$$\n",
    "\\mu_{\\text{run}}^{(1)} = [0.5, \\, 1.1], \n",
    "\\quad\n",
    "\\sigma_{\\text{run}}^{2,(1)} = [1, \\, 1].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Batch #2\n",
    "\n",
    "- **Given:**  \n",
    "  - $\\mu_{\\text{run}}^{(1)} = [0.5, \\, 1.1]$  \n",
    "  - $\\sigma_{\\text{run}}^{2,(1)} = [1, \\, 1]$  \n",
    "  - (Same mini-batch) $\\mu_{\\text{batch}} = [5, \\, 11]$  \n",
    "  - (Same mini-batch) $\\sigma_{\\text{batch}}^2 = [1, \\, 1]$\n",
    "\n",
    "1. **Update Running Mean**  \n",
    "   $$\n",
    "   \\mu_{\\text{run}}^{(2)}\n",
    "   = 0.9 \\,[0.5, \\, 1.1]\n",
    "   \\;+\\; 0.1 \\,[5, \\, 11].\n",
    "   $$\n",
    "   Calculate each dimension:\n",
    "   - First feature:\n",
    "     $$\n",
    "     0.9 \\times 0.5 = 0.45,\n",
    "     \\quad\n",
    "     0.1 \\times 5 = 0.5,\n",
    "     \\quad\n",
    "     \\text{sum} = 0.95.\n",
    "     $$\n",
    "   - Second feature:\n",
    "     $$\n",
    "     0.9 \\times 1.1 = 0.99,\n",
    "     \\quad\n",
    "     0.1 \\times 11 = 1.1,\n",
    "     \\quad\n",
    "     \\text{sum} = 2.09.\n",
    "     $$\n",
    "   Hence:\n",
    "   $$\n",
    "   \\mu_{\\text{run}}^{(2)} = [0.95,\\; 2.09].\n",
    "   $$\n",
    "\n",
    "2. **Update Running Variance**  \n",
    "   $$\n",
    "   \\sigma_{\\text{run}}^{2,(2)}\n",
    "   = 0.9 \\,[1,\\, 1]\n",
    "   + 0.1 \\,[1,\\, 1]\n",
    "   = [1,\\, 1].\n",
    "   $$\n",
    "\n",
    "So after iteration 2:\n",
    "$$\n",
    "\\mu_{\\text{run}}^{(2)} = [0.95, \\, 2.09],\n",
    "\\quad\n",
    "\\sigma_{\\text{run}}^{2,(2)} = [1, \\, 1].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Batch #3\n",
    "\n",
    "- **Given:**  \n",
    "  - $\\mu_{\\text{run}}^{(2)} = [0.95, \\, 2.09]$  \n",
    "  - $\\sigma_{\\text{run}}^{2,(2)} = [1, \\, 1]$  \n",
    "  - (Same mini-batch) $\\mu_{\\text{batch}} = [5, \\, 11]$  \n",
    "  - (Same mini-batch) $\\sigma_{\\text{batch}}^2 = [1, \\, 1]$\n",
    "\n",
    "1. **Update Running Mean**  \n",
    "   $$\n",
    "   \\mu_{\\text{run}}^{(3)}\n",
    "   = 0.9 \\,[0.95,\\, 2.09]\n",
    "   + 0.1 \\,[5,\\, 11].\n",
    "   $$\n",
    "   Calculate each dimension:\n",
    "   - First feature:\n",
    "     $$\n",
    "     0.9 \\times 0.95 = 0.855,\n",
    "     \\quad\n",
    "     0.1 \\times 5 = 0.5,\n",
    "     \\quad\n",
    "     \\text{sum} = 1.355.\n",
    "     $$\n",
    "   - Second feature:\n",
    "     $$\n",
    "     0.9 \\times 2.09 = 1.881,\n",
    "     \\quad\n",
    "     0.1 \\times 11 = 1.1,\n",
    "     \\quad\n",
    "     \\text{sum} = 2.981.\n",
    "     $$\n",
    "   Hence:\n",
    "   $$\n",
    "   \\mu_{\\text{run}}^{(3)} \\approx [1.355,\\, 2.981].\n",
    "   $$\n",
    "\n",
    "2. **Update Running Variance**  \n",
    "   $$\n",
    "   \\sigma_{\\text{run}}^{2,(3)}\n",
    "   = 0.9 \\,[1,\\,1]\n",
    "   + 0.1 \\,[1,\\,1]\n",
    "   = [1,\\,1].\n",
    "   $$\n",
    "\n",
    "So after iteration 3:\n",
    "$$\n",
    "\\mu_{\\text{run}}^{(3)} \\approx [1.355, \\, 2.981],\n",
    "\\quad\n",
    "\\sigma_{\\text{run}}^{2,(3)} = [1, \\, 1].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Observations\n",
    "\n",
    "1. **Running Mean Convergence:**  \n",
    "   Each iteration, $\\mu_{\\text{run}}$ is nudged closer to the true mini-batch mean $[5,\\,11]$. If you continue many more iterations (with the same mini-batch), $\\mu_{\\text{run}}$ will gradually converge toward $[5,\\, 11]$.\n",
    "\n",
    "2. **Running Variance Remains at [1, 1]:**  \n",
    "   Because our mini-batch variance is consistently $[1,\\,1]$ and our initial running variance was also $[1,\\,1]$, the update formula keeps it at $[1,\\,1]$. In practice, if the batch variance or initial guess differed, you’d see the variance slowly move toward the actual values.\n",
    "\n",
    "3. **Practical Use at Inference:**  \n",
    "   Once training is finished, we use the final stored $\\mu_{\\text{run}}$ and $\\sigma_{\\text{run}}^2$ to normalize any new data during testing/inference:\n",
    "   $$\n",
    "   \\hat{x}_{\\text{test}} \n",
    "   = \\frac{x_{\\text{test}} - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\n",
    "   $$\n",
    "   which avoids computing statistics on potentially small or single test samples.\n",
    "\n",
    "Thus, this example shows how the running statistics evolve iteration by iteration for **two-feature** data in a consistent mini-batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Testing Works in Batch Normalization\n",
    "\n",
    "When you train a neural network with Batch Normalization (BatchNorm), **for each mini-batch** of data you:\n",
    "1. Compute the **mini-batch mean** ($\\mu_{\\text{batch}}$) and **mini-batch variance** ($\\sigma_{\\text{batch}}^2$).\n",
    "2. Use these values to **normalize** your activations.\n",
    "3. **Update** the *running* (or *moving*) mean $\\mu_{\\text{run}}$ and variance $\\sigma_{\\text{run}}^2$.\n",
    "\n",
    "However, **at test (inference) time**, you typically pass **one** sample at a time (or very small batches). Computing a new mean/variance from that single test sample wouldn’t be stable. \n",
    "\n",
    "### The Key Idea\n",
    "\n",
    "- **During training:**\n",
    "  - We keep track of an exponential-moving average (EMA) of the batch mean and variance over all iterations. These become our **running mean** ($\\mu_{\\text{run}}$) and **running variance** ($\\sigma_{\\text{run}}^2$).\n",
    "  - In pseudo-code:\n",
    "    $$\n",
    "    \\mu_{\\text{run}} \\leftarrow \\alpha \\,\\mu_{\\text{run}} + (1-\\alpha)\\,\\mu_{\\text{batch}}\n",
    "    $$\n",
    "    $$\n",
    "    \\sigma_{\\text{run}}^2 \\leftarrow \\alpha \\,\\sigma_{\\text{run}}^2 + (1-\\alpha)\\,\\sigma_{\\text{batch}}^2\n",
    "    $$\n",
    "    where $\\alpha$ is often something like $0.9$ or $0.99$.\n",
    "\n",
    "- **At test time:**\n",
    "  - We **do not** compute $\\mu_{\\text{batch}}$ or $\\sigma_{\\text{batch}}^2$ from the test sample(s).\n",
    "  - Instead, we use the final stored $\\mu_{\\text{run}}$ and $\\sigma_{\\text{run}}^2$ from training.\n",
    "  - This ensures stability and consistency, because $\\mu_{\\text{run}}$ and $\\sigma_{\\text{run}}^2$ reflect the *overall distribution* of training data.\n",
    "\n",
    "Formally, if $x_{\\text{test}}$ is a test example (or a small batch), the BatchNorm normalization step at test time is:\n",
    "\n",
    "$$\n",
    "\\hat{x}_{\\text{test}} \n",
    "= \\frac{x_{\\text{test}} - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Then we apply the learned scale and shift:\n",
    "\n",
    "$$\n",
    "y_{\\text{test}} \n",
    "= \\gamma \\,\\hat{x}_{\\text{test}} + \\beta\n",
    "$$\n",
    "\n",
    "Here, $\\gamma$ and $\\beta$ are the same **trainable parameters** used during training.\n",
    "\n",
    "### Why Not Use the Test Sample’s Mean and Variance?\n",
    "\n",
    "- A single test sample (or a very small test batch) won’t give a reliable estimate of the mean or variance.\n",
    "- If you tried to compute the mean/variance of one sample, that sample would end up normalized to zero mean and unit variance all by itself—completely changing the model’s learned expectations.\n",
    "- Using **running** (EMA) statistics from training gives the best estimate of how the data is expected to be distributed.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Training:** Compute mean/variance from the current mini-batch, update the *running average* for future use.\n",
    "- **Testing:** Use the *stored* running mean/variance (no new computation from the test data).  \n",
    "\n",
    "That’s how BatchNorm remains consistent even when test batches are small (or just single images, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
