{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization\n",
    "\n",
    "Layer normalization is a technique used to normalize the inputs across the features for each data point, improving the training stability and performance of neural networks.\n",
    "\n",
    "## Formula\n",
    "\n",
    "The layer normalization process can be described by the following equation:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$\n",
    "\\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2}\n",
    "$$\n",
    "\n",
    "- $x$ is the input vector.\n",
    "- $\\mu$ is the mean of the input.\n",
    "- $\\sigma$ is the standard deviation of the input.\n",
    "- $\\epsilon$ is a small constant for numerical stability.\n",
    "- $H$ is the number of hidden units in a layer (number of features)\n",
    "- $\\gamma$ and $\\beta$ are learnable parameters that scale and shift the normalized output.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{1. Improved training stability} \\\\\n",
    "\\text{2. Faster convergence} \\\\\n",
    "\\text{3. Reduced sensitivity to initialization}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Layer normalization is particularly effective in recurrent neural networks and transformer architectures, where it helps in managing the internal covariate shift.\n",
    "\n",
    "## Comparison with Batch Normalization\n",
    "\n",
    "Unlike batch normalization, layer normalization does not depend on the batch size and can be applied to individual samples, making it more suitable for certain applications.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Batch Normalization:} & \\quad \\text{Normalizes across the batch} \\\\\n",
    "\\text{Layer Normalization:} & \\quad \\text{Normalizes across the features}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma and Beta Parameters in Layer Normalization\n",
    "\n",
    "The $\\gamma$ and $\\beta$ parameters in layer normalization are applied using element-wise operations.\n",
    "\n",
    "## Explanation\n",
    "\n",
    "In layer normalization, after normalizing the input, the $\\gamma$ and $\\beta$ parameters are used to scale and shift the normalized output. These parameters are applied element-wise, meaning each feature in the input vector is scaled and shifted individually.\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "Here, $\\gamma$ and $\\beta$ are vectors of the same dimension as $x$, and the multiplication and addition are performed element-wise.\n",
    "\n",
    "## Element-wise Operation\n",
    "\n",
    "Element-wise operations ensure that each feature is independently scaled and shifted, allowing the model to learn the optimal scale and shift for each feature during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization\n",
    "\n",
    "Layer normalization is a technique used to stabilize and accelerate the training of deep neural networks. Unlike batch normalization, which normalizes across the batch dimension, layer normalization normalizes the inputs across the features for each individual data sample. This makes it particularly useful for recurrent neural networks and other architectures where batch normalization may not be as effective.\n",
    "\n",
    "Mathematically, for a given input vector $\\mathbf{x} = (x_1, x_2, \\dots, x_H)$, layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LN}(\\mathbf{x}) = \\gamma \\cdot \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i$ is the mean of the input features.\n",
    "- $\\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2$ is the variance of the input features.\n",
    "- $\\gamma$ and $\\beta$ are learnable parameters that scale and shift the normalized output.\n",
    "- $\\epsilon$ is a small constant added for numerical stability.\n",
    "\n",
    "# Covariate Shift in Neural Networks\n",
    "\n",
    "Covariate shift refers to the change in the distribution of input data $\\mathbf{x}$ between the training and testing phases, while the conditional distribution $P(y|\\mathbf{x})$ remains the same. In the context of neural networks, covariate shift can lead to degraded performance because the model has been trained on a different data distribution than it encounters during deployment.\n",
    "\n",
    "Formally, covariate shift occurs when:\n",
    "\n",
    "$$\n",
    "P_{\\text{train}}(\\mathbf{x}) \\neq P_{\\text{test}}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "but\n",
    "\n",
    "$$\n",
    "P_{\\text{train}}(y|\\mathbf{x}) = P_{\\text{test}}(y|\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Addressing covariate shift typically involves techniques such as domain adaptation, importance weighting, or normalization methods like layer normalization to ensure that the model remains robust to changes in the input distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding Correlated Changes in Neural Networks\n",
    "\n",
    "To grasp the statement:\n",
    "\n",
    "> \"Notice that changes in the output of one layer will tend to cause highly correlated changes in the summed inputs to the next layer, especially with ReLU units whose outputs can change by a lot. This suggests the 'covariate shift' problem can be reduced by fixing the mean and the variance of the summed inputs within each layer.\"\n",
    "\n",
    "let's break it down step by step.\n",
    "\n",
    "## Layer Outputs and Their Impact\n",
    "\n",
    "In a neural network, each layer processes inputs and produces outputs that serve as inputs to the next layer. When the output of one layer changes, it directly affects the inputs of the subsequent layer.\n",
    "\n",
    "### Example with ReLU Activation\n",
    "\n",
    "Consider a layer that uses the ReLU (Rectified Linear Unit) activation function, defined as:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "ReLU introduces non-linearity by zeroing out negative values and keeping positive values unchanged. This characteristic can lead to significant changes in the output, especially when the input varies.\n",
    "\n",
    "## Correlated Changes in Summed Inputs\n",
    "\n",
    "When the output of a layer changes, especially with activation functions like ReLU, it can cause **highly correlated changes** in the summed inputs to the next layer. Here's why:\n",
    "\n",
    "1. **Activation Variability**: ReLU can turn many inputs to zero or keep them as they are. Small changes in the input can lead to large changes in the output (from positive to zero or vice versa).\n",
    "\n",
    "2. **Summation Effect**: The next layer typically sums the weighted inputs from the previous layer. If multiple outputs from the previous layer change in a correlated manner, their summed input to the next layer will also change significantly.\n",
    "\n",
    "### Mathematical Illustration\n",
    "\n",
    "Suppose the output of layer $\\l$ is a vector $\\mathbf{h}^{l} = (h_1^{l}, h_2^{l}, \\dots, h_n^{l})$, where each $\\h_i^{(l)}$ is the result of applying ReLU to the weighted sum of inputs:\n",
    "\n",
    "$$\n",
    "h_i^{(l)} = \\text{ReLU}\\left(\\sum_{j} w_{ij}^{(l)} x_j^{(l)} + b_i^{(l)}\\right)\n",
    "$$\n",
    "\n",
    "If the outputs $\\ h_i^{(l)}z4 change significantly, the input to the next layer \\( l+1 \\), which is often a weighted sum:\n",
    "\n",
    "$$\n",
    "x_i^{(l+1)} = \\sum_{k} v_{ik}^{(l+1)} h_k^{(l)}\n",
    "$$\n",
    "\n",
    "will also change significantly. If multiple $h_k^{(l)}$ change in a similar way (e.g., many become zero or many increase), the summed input \\( x_i^{(l+1)} \\) will exhibit correlated changes.\n",
    "\n",
    "## Covariate Shift Problem\n",
    "\n",
    "**Covariate shift** occurs when the distribution of inputs to a layer changes during training versus testing. In the context of neural networks:\n",
    "\n",
    "- **Training Phase**: The network learns weights based on the distribution of inputs it sees.\n",
    "- **Testing Phase**: If the input distribution shifts, the learned weights may not perform as well, leading to degraded performance.\n",
    "\n",
    "The correlated changes in summed inputs exacerbate covariate shift because they make the input distribution to each layer more volatile and less stable.\n",
    "\n",
    "## Mitigating Covariate Shift with Normalization\n",
    "\n",
    "To address covariate shift, it's beneficial to stabilize the input distributions of each layer. One effective method is **Layer Normalization**, which fixes the mean and variance of the summed inputs within each layer.\n",
    "\n",
    "### How Layer Normalization Helps\n",
    "\n",
    "Layer normalization adjusts the inputs to have a consistent mean and variance, reducing the variability caused by activation functions like ReLU. This stabilization leads to:\n",
    "\n",
    "- **Reduced Correlation**: By fixing the mean and variance, the inputs to each layer become less sensitive to changes in the previous layer's outputs.\n",
    "- **Improved Training Stability**: Consistent input distributions help in more stable and faster convergence during training.\n",
    "- **Better Generalization**: Models are less likely to overfit to specific input distributions, enhancing performance on unseen data.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Layer normalization transforms the input vector \\( \\mathbf{x} \\) as follows:\n",
    "\n",
    "$$\n",
    "\\text{LN}(\\mathbf{x}) = \\gamma \\cdot \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i$  is the mean of the input features.\n",
    "- $\\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2$ is the variance of the input features.\n",
    "- $\\gamma$ and $\\beta$ are learnable parameters that scale and shift the normalized output.\n",
    "- $\\epsilon$ is a small constant added for numerical stability.\n",
    "\n",
    "By applying this normalization, each layer's inputs maintain a stable distribution, mitigating the effects of covariate shift and ensuring that changes in one layer do not cause unpredictable changes in subsequent layers.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Correlated Changes**: Changes in one layer's output can lead to significant and correlated changes in the next layer's inputs, especially with activation functions like ReLU.\n",
    "- **Covariate Shift**: These correlated changes can cause the input distribution to shift between training and testing, harming model performance.\n",
    "- **Layer Normalization**: By fixing the mean and variance of inputs within each layer, layer normalization stabilizes the input distributions, reducing covariate shift and enhancing the network's robustness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
