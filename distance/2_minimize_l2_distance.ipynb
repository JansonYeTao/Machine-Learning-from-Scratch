{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gradient of Sum of L2 Norm Distance#\n",
    "\n",
    "Let the list of points be \\( \\{x_1, x_2, \\dots, x_n\\} \\) and the variable \\( y \\) be the point that minimizes the sum of L2 norms. The objective function can be written as:\n",
    "\n",
    "$$\n",
    "f(y) = \\sum_{i=1}^{n} \\|y - x_i\\|_2\n",
    "$$\n",
    "\n",
    "To find the gradient, we consider the derivative of \\( f(y) \\) with respect to \\( y \\). The gradient is given by:\n",
    "\n",
    "$$\n",
    "\\nabla f(y) = \\sum_{i=1}^{n} \\frac{y - x_i}{\\|y - x_i\\|_2}\n",
    "$$\n",
    "\n",
    "This gradient will guide the optimization process to minimize the sum of L2 norms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the Sum of L2 Norm Distance\n",
    "\n",
    "Let the list of points be denoted as $\\{x_1, x_2, \\ldots, x_n\\}$, where each $x_i \\in \\mathbb{R}^d$. We aim to find a point $y \\in \\mathbb{R}^d$ that minimizes the sum of the L2 norms of the distances between $y$ and each $x_i$. The objective function can be written as:\n",
    "\n",
    "$$\n",
    "f(y) = \\sum_{i=1}^n \\|y - x_i\\|_2\n",
    "$$\n",
    "\n",
    "### Step 1: Expanding the L2 Norm\n",
    "The L2 norm $\\|y - x_i\\|_2$ is defined as:\n",
    "\n",
    "$$\n",
    "\\|y - x_i\\|_2 = \\sqrt{(y - x_i)^\\top (y - x_i)}\n",
    "$$\n",
    "\n",
    "Substituting into $f(y)$:\n",
    "\n",
    "$$\n",
    "f(y) = \\sum_{i=1}^n \\sqrt{(y - x_i)^\\top (y - x_i)}\n",
    "$$\n",
    "\n",
    "### Step 2: Gradient of the Objective Function\n",
    "The gradient of $f(y)$ with respect to $y$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla f(y) = \\sum_{i=1}^n \\nabla \\|y - x_i\\|_2\n",
    "$$\n",
    "\n",
    "Using the chain rule, the gradient of $\\|y - x_i\\|_2$ is:\n",
    "\n",
    "$$\n",
    "\\nabla \\|y - x_i\\|_2 = \\frac{y - x_i}{\\|y - x_i\\|_2}\n",
    "$$\n",
    "\n",
    "Thus, the gradient of $f(y)$ becomes:\n",
    "\n",
    "$$\n",
    "\\nabla f(y) = \\sum_{i=1}^n \\frac{y - x_i}{\\|y - x_i\\|_2}\n",
    "$$\n",
    "\n",
    "### Step 3: Induction of the Minimizer\n",
    "To minimize $f(y)$, we solve for $y$ such that $\\nabla f(y) = 0$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\frac{y - x_i}{\\|y - x_i\\|_2} = 0\n",
    "$$\n",
    "\n",
    "This condition implies that the optimal $y$ is a weighted median of the points $\\{x_i\\}$, where the weights depend inversely on the distances $\\|y - x_i\\|_2$.\n",
    "\n",
    "### Conclusion\n",
    "The gradient $\\nabla f(y)$ provides the direction to adjust $y$ iteratively to minimize the sum of L2 norms. The minimization leads to a point $y$ that balances the distances to all points $x_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_point: [1 2]\n",
      "final point: [3.00000004 4.00000004]\n"
     ]
    }
   ],
   "source": [
    "def objective_function(y, points):\n",
    "    # square of l2 distance\n",
    "    return np.sum(np.linalg.norm((points - y)**2, axis=1))\n",
    "\n",
    "points = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "initial_point = points[0]\n",
    "print(f\"initial_point: {initial_point}\")\n",
    "loss = objective_function(initial_point, points)\n",
    "results = minimize(fun=objective_function, x0=initial_point, args=(points, ))\n",
    "print(f\"final point: {results.x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_point: [1 2]\n",
      "final point: [2.99999999 3.99999999]\n"
     ]
    }
   ],
   "source": [
    "def objective_function(y, points):\n",
    "    # l2 distance\n",
    "    return np.sum(np.linalg.norm(points - y, axis=1))\n",
    "\n",
    "points = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "\n",
    "initial_point = points[0]\n",
    "print(f\"initial_point: {initial_point}\")\n",
    "loss = objective_function(initial_point, points)\n",
    "results = minimize(fun=objective_function, x0=initial_point, args=(points, ))\n",
    "print(f\"final point: {results.x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    def __call__(self, y, X):\n",
    "\n",
    "        \"\"\"\n",
    "        l = |(y - X)|^(1/2)\n",
    "        dl/dy = \n",
    "        (1/2) *  |(y-X)|**2 / |(y - X)|\n",
    "        \"\"\"\n",
    "\n",
    "        val = np.sum(np.linalg.norm(y - X, axis=1))\n",
    "        return val\n",
    "        \n",
    "    def backward(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\#\n",
    "Step-by-Step Induction for the Gradient of Sum of L2 Norms\n",
    "\\#\n",
    "\n",
    "Let the list of points be \\( \\{x_1, x_2, \\dots, x_n\\} \\) and the variable \\( y \\) be the point that minimizes the sum of L2 norms. The objective function is:\n",
    "\n",
    "$$\n",
    "f(y) = \\sum_{i=1}^n \\|y - x_i\\|_2\n",
    "$$\n",
    "\n",
    "### Step 1: Expand the L2 Norm\n",
    "The L2 norm for a point \\( x_i \\) is defined as:\n",
    "\n",
    "$$\n",
    "\\|y - x_i\\|_2 = \\sqrt{\\sum_{j=1}^d (y_j - x_{i,j})^2}\n",
    "$$\n",
    "\n",
    "where \\( y = (y_1, y_2, \\dots, y_d) \\) and \\( x_i = (x_{i,1}, x_{i,2}, \\dots, x_{i,d}) \\).\n",
    "\n",
    "Thus, the function \\( f(y) \\) becomes:\n",
    "\n",
    "$$\n",
    "f(y) = \\sum_{i=1}^n \\sqrt{\\sum_{j=1}^d (y_j - x_{i,j})^2}\n",
    "$$\n",
    "\n",
    "### Step 2: Differentiate the L2 Norm\n",
    "To find the gradient of \\( f(y) \\), we differentiate each term \\( \\|y - x_i\\|_2 \\) with respect to \\( y \\). Using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y_j} \\|y - x_i\\|_2 = \\frac{\\partial}{\\partial y_j} \\sqrt{\\sum_{k=1}^d (y_k - x_{i,k})^2} \n",
    "= \\frac{1}{2 \\sqrt{\\sum_{k=1}^d (y_k - x_{i,k})^2}} \\cdot 2(y_j - x_{i,j})\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y_j} \\|y - x_i\\|_2 = \\frac{y_j - x_{i,j}}{\\|y - x_i\\|_2}\n",
    "$$\n",
    "\n",
    "### Step 3: Gradient for All Dimensions\n",
    "The gradient of \\( \\|y - x_i\\|_2 \\) with respect to \\( y \\) (a vector) is then:\n",
    "\n",
    "$$\n",
    "\\nabla \\|y - x_i\\|_2 = \\frac{y - x_i}{\\|y - x_i\\|_2}\n",
    "$$\n",
    "\n",
    "### Step 4: Sum Over All Points\n",
    "Finally, the gradient of \\( f(y) \\) is the sum of the gradients of each term:\n",
    "\n",
    "$$\n",
    "\\nabla f(y) = \\sum_{i=1}^n \\nabla \\|y - x_i\\|_2 = \\sum_{i=1}^n \\frac{y - x_i}{\\|y - x_i\\|_2}\n",
    "$$\n",
    "\n",
    "Thus, the gradient of the sum of L2 norms is:\n",
    "\n",
    "$$\n",
    "\\nabla f(y) = \\sum_{i=1}^n \\frac{y - x_i}{\\|y - x_i\\|_2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\text{To compute the derivative, the following steps are missing:} \\\\\n",
    "\n",
    "\\text{1. Expand the derivative explicitly:} \\\\\n",
    "\\frac{\\partial}{\\partial y_j} \\sqrt{\\sum_{k=1}^d (y_k - x_{i,k})^2} \n",
    "= \\frac{\\partial}{\\partial y_j} \\Big( \\big( \\sum_{k=1}^d (y_k - x_{i,k})^2 \\big)^{\\frac{1}{2}} \\Big)\n",
    "\n",
    "\\\\\n",
    "\\text{2. Apply the chain rule:} \\\\\n",
    "\\frac{\\partial}{\\partial y_j} \\Big( \\big( \\sum_{k=1}^d (y_k - x_{i,k})^2 \\big)^{\\frac{1}{2}} \\Big)\n",
    "= \\frac{1}{2} \\Big( \\sum_{k=1}^d (y_k - x_{i,k})^2 \\Big)^{-\\frac{1}{2}} \\cdot \\frac{\\partial}{\\partial y_j} \\Big( \\sum_{k=1}^d (y_k - x_{i,k})^2 \\Big)\n",
    "\n",
    "\\\\\n",
    "\\text{3. Simplify the derivative of the inner sum:} \\\\\n",
    "\\frac{\\partial}{\\partial y_j} \\Big( \\sum_{k=1}^d (y_k - x_{i,k})^2 \\Big)\n",
    "= 2(y_j - x_{i,j})\n",
    "\n",
    "\\\\\n",
    "\\text{4. Substitute the simplified derivative back:} \\\\\n",
    "\\frac{\\partial}{\\partial y_j} \\sqrt{\\sum_{k=1}^d (y_k - x_{i,k})^2} \n",
    "= \\frac{1}{2} \\Big( \\sum_{k=1}^d (y_k - x_{i,k})^2 \\Big)^{-\\frac{1}{2}} \\cdot 2(y_j - x_{i,j})\n",
    "\n",
    "\\\\\n",
    "\\text{5. Final simplification:} \\\\\n",
    "\\frac{\\partial}{\\partial y_j} \\sqrt{\\sum_{k=1}^d (y_k - x_{i,k})^2} \n",
    "= \\frac{(y_j - x_{i,j})}{\\sqrt{\\sum_{k=1}^d (y_k - x_{i,k})^2}}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal y: [2.99660991 4.00347191]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent_l2(points, y_init, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to minimize the sum of L2 norms.\n",
    "    \n",
    "    Args:\n",
    "        points (numpy.ndarray): Array of shape (n, d), where n is the number of points and d is the dimension.\n",
    "        y_init (numpy.ndarray): Initial guess for y, of shape (d,).\n",
    "        learning_rate (float): Step size for gradient descent.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        tol (float): Tolerance for stopping criterion.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The optimal y that minimizes the sum of L2 norms.\n",
    "    \"\"\"\n",
    "    y = y_init.copy()\n",
    "    for i in range(max_iter):\n",
    "        # Compute the gradient\n",
    "        gradient = np.sum([(y - x) / np.linalg.norm(y - x) for x in points], axis=0)\n",
    "        \n",
    "        # Update y\n",
    "        y_new = y - learning_rate * gradient\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(y_new - y) < tol:\n",
    "            print(f\"Converged in {i+1} iterations.\")\n",
    "            break\n",
    "        \n",
    "        y = y_new\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Example usage\n",
    "points = np.array([[1, 2], [3, 4], [5, 6]])  # List of points (n=3, d=2)\n",
    "y_init = np.array([0.0, 0.0])  # Initial guess for y\n",
    "optimal_y = gradient_descent_l2(points, y_init)\n",
    "print(\"Optimal y:\", optimal_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
