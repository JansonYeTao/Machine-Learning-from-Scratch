{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z$ is the input to the sigmoid function.\n",
    "\n",
    "The loss function, commonly using binary cross-entropy, is:\n",
    "\n",
    "$$\n",
    "L = -\\left( y \\log(\\sigma(z)) + (1 - y) \\log(1 - \\sigma(z)) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Here:\n",
    "- $y$ represents the true label.\n",
    "- $\\sigma(z)$ is the predicted probability from the sigmoid function.\n",
    "\n",
    "To calculate the gradient of the loss with respect to $z$, we differentiate $L$ with respect to $z$:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dz} = \\sigma(z) - y\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "L &= -\\left( y \\log(\\sigma(z)) + (1 - y) \\log(1 - \\sigma(z)) \\right) \\\\\n",
    "\\frac{dL}{d\\sigma(z)} &= -\\left( \\frac{y}{\\sigma(z)} - \\frac{1 - y}{1 - \\sigma(z)} \\right) \\\\\n",
    "\\frac{d\\sigma(z)}{dz} &= \\sigma(z) (1 - \\sigma(z)) \\\\\n",
    "\\frac{dL}{dz} &= \\frac{dL}{d\\sigma(z)} \\cdot \\frac{d\\sigma(z)}{dz} \\\\\n",
    "&= -\\left( \\frac{y}{\\sigma(z)} - \\frac{1 - y}{1 - \\sigma(z)} \\right) \\cdot \\sigma(z) (1 - \\sigma(z)) \\\\\n",
    "&= -\\left( y (1 - \\sigma(z)) - (1 - y) \\sigma(z) \\right) \\\\\n",
    "&= \\sigma(z) - y\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "**Notation:**\n",
    "- $z$: Input to the sigmoid function.\n",
    "- $y$: True label (target).\n",
    "- $\\sigma(z)$: Output of the sigmoid function, representing the predicted probability.\n",
    "- $L$: Loss function (binary cross-entropy).\n",
    "- $\\frac{dL}{dz}$: Gradient of the loss with respect to $z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for $z$ is given by:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "**Notation:**\n",
    "- $\\mathbf{w}$: Weight vector.\n",
    "- $\\mathbf{x}$: Input feature vector.\n",
    "- $b$: Bias term.\n",
    "- $z$: Linear combination of inputs and weights plus bias.\n",
    "\n",
    "**Explanation:**\n",
    "- $\\mathbf{w}^\\top \\mathbf{x}$ represents the dot product of the weight vector and the input feature vector.\n",
    "- $b$ is the bias that allows the model to adjust the output along with the weighted input features.\n",
    "- Together, $z$ serves as the input to the sigmoid function $\\sigma(z)$, which produces the predicted probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients of the loss function $L$ with respect to the weight vector $\\mathbf{w}$ and the bias $b$ are defined as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = (\\sigma(z) - y) \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\sigma(z) - y\n",
    "$$\n",
    "\n",
    "**Notation:**\n",
    "- $\\mathbf{w}$: Weight vector.\n",
    "- $b$: Bias term.\n",
    "- $\\mathbf{x}$: Input feature vector.\n",
    "- $y$: True label.\n",
    "- $\\sigma(z)$: Output of the sigmoid function.\n",
    "- $L$: Loss function (binary cross-entropy).\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "Given the loss function:\n",
    "$$\n",
    "L = -\\left( y \\log(\\sigma(z)) + (1 - y) \\log(1 - \\sigma(z)) \\right)\n",
    "$$\n",
    "\n",
    "And the linear combination:\n",
    "$$\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "We have already established that:\n",
    "$$\n",
    "\\frac{dL}{dz} = \\sigma(z) - y\n",
    "$$\n",
    "\n",
    "To find the gradients with respect to $\\mathbf{w}$ and $b$, we apply the chain rule.\n",
    "\n",
    "1. **Gradient with respect to the weight vector $\\mathbf{w}$:**\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{dL}{dz} \\cdot \\frac{\\partial z}{\\partial \\mathbf{w}} = (\\sigma(z) - y) \\mathbf{x}\n",
    "   $$\n",
    "\n",
    "   - **Explanation:** The gradient with respect to $\\mathbf{w}$ is the product of the error term $(\\sigma(z) - y)$ and the input feature vector $\\mathbf{x}$.\n",
    "\n",
    "2. **Gradient with respect to the bias $b$:**\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial b} = \\frac{dL}{dz} \\cdot \\frac{\\partial z}{\\partial b} = \\sigma(z) - y\n",
    "   $$\n",
    "\n",
    "   - **Explanation:** The gradient with respect to $b$ is simply the error term $(\\sigma(z) - y)$ since the derivative of $z$ with respect to $b$ is 1.\n",
    "\n",
    "**Summary:**\n",
    "- The gradient with respect to the weights $\\mathbf{w}$ is proportional to the input features scaled by the prediction error.\n",
    "- The gradient with respect to the bias $b$ is equal to the prediction error.\n",
    "\n",
    "These gradients are used in optimization algorithms like Gradient Descent to update the model parameters $\\mathbf{w}$ and $b$ in order to minimize the loss function $L$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    y_pred = 1 / (1 + np.exp(-z)) # element-wise process\n",
    "    return y_pred\n",
    "\n",
    "class BCELoss: \n",
    "    \"\"\"\n",
    "    Binary cross entropy loss\n",
    "    p if y = 1\n",
    "    1- p if y = 0\n",
    "\n",
    "    loss = p**y * (1-p)**(1-y) -> ylogp + (1-y)log(1-p)\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        loss = -(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred)) # do forget the negative sign\n",
    "        self.y_true, self.y_pred = y_true, y_pred\n",
    "        return np.mean(loss) # mean average the loss vector (loss for each row sample) and aggregate\n",
    "    \n",
    "    def get_loss_grad(self):\n",
    "        \"\"\"\n",
    "        calculate gradient\n",
    "        loss gradient = X_transpose @ (y_pred - y_true)\n",
    "        # more detail here: https://classic.d2l.ai/chapter_linear-networks/softmax-regression.html#softmax-and-derivatives]\n",
    "        # notice for softmax/linear regression, the loss grad is the same thing here and this is not a coincidence\n",
    "        \"\"\"\n",
    "        loss_grad = (self.y_pred - self.y_true) / self.y_true.shape[0]\n",
    "        return loss_grad\n",
    "\n",
    "\n",
    "class LogisticModel:\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weight = np.random.randn(input_dim, output_dim)\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        # self.bias = np.zeros([0]): typo here which results in [] loss\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        X: (n, p), w: (p, 1)\n",
    "        y: (n, 1)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        z = X @ self.weight + self.bias\n",
    "        y_pred = sigmoid(z)\n",
    "        return y_pred\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        \"\"\"calculate gradient\n",
    "        loss gradient = X_transpose @ (y_pred - y_true)\n",
    "        \"\"\"\n",
    "        self.weight_grad = self.X.T @ loss_grad \n",
    "        self.bias_grad = loss_grad.sum(axis=0)\n",
    "\n",
    "    def step(self, lr=0.01):\n",
    "        \"\"\"gradient descent\"\"\"\n",
    "        self.weight = self.weight - self.weight_grad * lr \n",
    "        self.bias = self.bias - self.bias_grad * lr \n",
    "    \n",
    "\n",
    "def one_epoch_train(X, y_true, epoch, model, loss_func):\n",
    "    y_pred = model(X)\n",
    "    loss = loss_func(y_true, y_pred)\n",
    "    if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}, Weight: {model.weight[0][0]:.4f}, Bias: {model.bias[0][0]:.4f}\")\n",
    "    loss_grad = loss_func.get_loss_grad()\n",
    "    model.backward(loss_grad)\n",
    "    model.step()\n",
    "\n",
    "\n",
    "\n",
    "def generate_data():\n",
    "    n, d = 400, 1\n",
    "    w_true, b_true = np.array([3]).reshape(-1, 1), np.array([0])\n",
    "    X = np.random.uniform(-1, 1, (n, d)) # (n, p)\n",
    "    y_prob = sigmoid(X @ w_true + b_true)\n",
    "    y_true = (y_prob >= 0.5).astype(int)\n",
    "    return X, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1) (400, 1)\n",
      "Epoch 0, Loss: 0.5292, Weight: 0.7566, Bias: 0.0000\n",
      "Epoch 10, Loss: 0.5258, Weight: 0.7752, Bias: 0.0009\n",
      "Epoch 20, Loss: 0.5223, Weight: 0.7937, Bias: 0.0017\n",
      "Epoch 30, Loss: 0.5190, Weight: 0.8121, Bias: 0.0026\n",
      "Epoch 40, Loss: 0.5156, Weight: 0.8303, Bias: 0.0035\n",
      "Epoch 50, Loss: 0.5124, Weight: 0.8484, Bias: 0.0043\n",
      "Epoch 60, Loss: 0.5091, Weight: 0.8663, Bias: 0.0051\n",
      "Epoch 70, Loss: 0.5060, Weight: 0.8841, Bias: 0.0060\n",
      "Epoch 80, Loss: 0.5028, Weight: 0.9018, Bias: 0.0068\n",
      "Epoch 90, Loss: 0.4997, Weight: 0.9194, Bias: 0.0076\n",
      "Epoch 100, Loss: 0.4967, Weight: 0.9368, Bias: 0.0084\n",
      "Epoch 110, Loss: 0.4937, Weight: 0.9541, Bias: 0.0093\n",
      "Epoch 120, Loss: 0.4907, Weight: 0.9713, Bias: 0.0101\n",
      "Epoch 130, Loss: 0.4878, Weight: 0.9883, Bias: 0.0109\n",
      "Epoch 140, Loss: 0.4850, Weight: 1.0053, Bias: 0.0116\n",
      "Epoch 150, Loss: 0.4821, Weight: 1.0221, Bias: 0.0124\n",
      "Epoch 160, Loss: 0.4793, Weight: 1.0388, Bias: 0.0132\n",
      "Epoch 170, Loss: 0.4766, Weight: 1.0554, Bias: 0.0140\n",
      "Epoch 180, Loss: 0.4739, Weight: 1.0719, Bias: 0.0148\n",
      "Epoch 190, Loss: 0.4712, Weight: 1.0882, Bias: 0.0155\n",
      "Epoch 200, Loss: 0.4685, Weight: 1.1045, Bias: 0.0163\n",
      "Epoch 210, Loss: 0.4659, Weight: 1.1206, Bias: 0.0170\n",
      "Epoch 220, Loss: 0.4634, Weight: 1.1366, Bias: 0.0178\n",
      "Epoch 230, Loss: 0.4608, Weight: 1.1525, Bias: 0.0185\n",
      "Epoch 240, Loss: 0.4583, Weight: 1.1683, Bias: 0.0192\n",
      "Epoch 250, Loss: 0.4558, Weight: 1.1840, Bias: 0.0200\n",
      "Epoch 260, Loss: 0.4534, Weight: 1.1996, Bias: 0.0207\n",
      "Epoch 270, Loss: 0.4510, Weight: 1.2151, Bias: 0.0214\n",
      "Epoch 280, Loss: 0.4486, Weight: 1.2305, Bias: 0.0221\n",
      "Epoch 290, Loss: 0.4463, Weight: 1.2458, Bias: 0.0228\n",
      "Epoch 300, Loss: 0.4440, Weight: 1.2610, Bias: 0.0235\n",
      "Epoch 310, Loss: 0.4417, Weight: 1.2760, Bias: 0.0242\n",
      "Epoch 320, Loss: 0.4395, Weight: 1.2910, Bias: 0.0249\n",
      "Epoch 330, Loss: 0.4372, Weight: 1.3059, Bias: 0.0256\n",
      "Epoch 340, Loss: 0.4350, Weight: 1.3207, Bias: 0.0263\n",
      "Epoch 350, Loss: 0.4329, Weight: 1.3354, Bias: 0.0270\n",
      "Epoch 360, Loss: 0.4307, Weight: 1.3500, Bias: 0.0276\n",
      "Epoch 370, Loss: 0.4286, Weight: 1.3645, Bias: 0.0283\n",
      "Epoch 380, Loss: 0.4265, Weight: 1.3790, Bias: 0.0290\n",
      "Epoch 390, Loss: 0.4245, Weight: 1.3933, Bias: 0.0296\n",
      "Epoch 400, Loss: 0.4225, Weight: 1.4075, Bias: 0.0303\n",
      "Epoch 410, Loss: 0.4204, Weight: 1.4217, Bias: 0.0309\n",
      "Epoch 420, Loss: 0.4185, Weight: 1.4358, Bias: 0.0316\n",
      "Epoch 430, Loss: 0.4165, Weight: 1.4497, Bias: 0.0322\n",
      "Epoch 440, Loss: 0.4146, Weight: 1.4636, Bias: 0.0329\n",
      "Epoch 450, Loss: 0.4127, Weight: 1.4774, Bias: 0.0335\n",
      "Epoch 460, Loss: 0.4108, Weight: 1.4912, Bias: 0.0341\n",
      "Epoch 470, Loss: 0.4089, Weight: 1.5048, Bias: 0.0347\n",
      "Epoch 480, Loss: 0.4071, Weight: 1.5184, Bias: 0.0354\n",
      "Epoch 490, Loss: 0.4052, Weight: 1.5318, Bias: 0.0360\n",
      "final acc: 0.995\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "X, y_true = generate_data()\n",
    "print(X.shape, y_true.shape)\n",
    "model = LogisticModel(1, 1)\n",
    "loss_func = BCELoss()\n",
    "\n",
    "for i in range(500):\n",
    "    one_epoch_train(X, y_true, i, model, loss_func)\n",
    "\n",
    "y_pred = model(X)\n",
    "acc = ((y_pred >= 0.5) == y_true).sum() / y_true.shape[0]\n",
    "print(f\"final acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
