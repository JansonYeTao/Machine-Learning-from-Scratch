{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See prerequisite material here for:\n",
    "- Softmax function: https://github.com/JansonYeTao/Machine-Learning-from-Scratch/blob/main/fundamental/activation/softmax.ipynb\n",
    "- Linear regression:https://github.com/JansonYeTao/Machine-Learning-from-Scratch/blob/main/models/linear_regression/linear_regression.ipynb\n",
    "- Logistic regression: https://github.com/JansonYeTao/Machine-Learning-from-Scratch/blob/main/models/logsitic_regression/logistic_regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Let the model be defined as:} \\quad z_i = \\mathbf{w}_i^\\top \\mathbf{x} + b_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Softmax function:} \\quad \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Loss function (Cross-Entropy Loss):} \\quad L = -\\sum_{i=1}^{K} y_i \\log(\\text{Softmax}(z_i))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Gradient of the loss with respect to } z_i:\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\text{Softmax}(z_i) - y_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Gradient with respect to the weight } \\mathbf{w}_i:\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}_i} = \\frac{\\partial L}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial \\mathbf{w}_i} = (\\text{Softmax}(z_i) - y_i) \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Gradient with respect to the bias } b_i:\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_i} = \\frac{\\partial L}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial b_i} = \\text{Softmax}(z_i) - y_i\n",
    "$$\n",
    "\n",
    "Summary of Gradients\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}_i} &= (\\text{Softmax}(z_i) - y_i) \\mathbf{x} \\\\\n",
    "\\frac{\\partial L}{\\partial b_i} &= \\text{Softmax}(z_i) - y_i\n",
    "\\end{aligned}\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the final results from above, the gradient w.r.t logit_j is actually the difference between y_pred (y_pred = softmax(o)) and y_true â†’ [image.png](https://www.notion.so/13d160e0a0f380bfb3eed5d2fed935a2?pvs=21). And if you the recall from logistic regression, theyâ€™re actually the same thing except y_pred = softmax(logit) instead of y_pred = sigmoid(logit) where logit = WX. And if you see linear regression regression gradient, the gradient is same (still be the difference) except y_pred = WX  here (No transformation function like softmax and sigmoid)\n",
    "\n",
    "<aside>\n",
    "ðŸ’¡\n",
    "\n",
    "Notice: Â This is not coincidence. In any exponential family (see theÂ [**online appendix on distributions**](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html)) model, the gradients of the log-likelihood are given by precisely this term. This fact makes computing gradients easy in practice.\n",
    "\n",
    "</aside>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do we need to calculate the jaccobian matrix for softmax gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Gradient of the Softmax Function\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Softmax}(z_i)}{\\partial z_j} =\n",
    "\\begin{cases}\n",
    "\\text{Softmax}(z_i) \\left(1 - \\text{Softmax}(z_i)\\right) & \\text{if } i = j \\\\\n",
    "- \\text{Softmax}(z_i) \\text{Softmax}(z_j) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{K} y_i \\log(\\text{Softmax}(z_i))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vector $\\mathbf{z} = [z_1, z_2, \\dots, z_K]$, the softmax function $\\sigma(\\mathbf{z})$ is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\quad \\text{for } i = 1, 2, \\dots, K\n",
    "$$\n",
    "\n",
    "\n",
    "To understand how to optimize models involving the softmax function, it's essential to compute its gradient with respect to the input vector $\\mathbf{z}$.\n",
    "\n",
    "### Jacobian Matrix\n",
    "\n",
    "The gradient of the softmax function is represented by the Jacobian matrix, which contains all first-order partial derivatives of the softmax outputs with respect to the inputs.\n",
    "\n",
    "For the softmax function $\\sigma(\\mathbf{z})$, the Jacobian matrix $J$ is:\n",
    "\n",
    "$$\n",
    "J_{ij} = \\frac{\\partial \\sigma(\\mathbf{z})_i}{\\partial z_j}\n",
    "$$\n",
    "\n",
    "### Computing the Gradient\n",
    "\n",
    "The partial derivative of the $i^{th}$ softmax output with respect to the $j^{th}$ input is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_i}{\\partial z_j} =\n",
    "\\begin{cases}\n",
    "\\sigma(\\mathbf{z})_i \\left(1 - \\sigma(\\mathbf{z})_i\\right) & \\text{if } i = j \\\\\n",
    "- \\sigma(\\mathbf{z})_i \\sigma(\\mathbf{z})_j & \\text{if } i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This can be compactly written as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_i}{\\partial z_j} = \\sigma(\\mathbf{z})_i \\left(\\delta_{ij} - \\sigma(\\mathbf{z})_j\\right)\n",
    "$$\n",
    "\n",
    "where $\\delta_{ij}$ is the Kronecker delta, which is 1 if $i = j$ and 0 otherwise.\n",
    "\n",
    "### Matrix Form\n",
    "\n",
    "In matrix form, the Jacobian $J$ can be expressed as:\n",
    "\n",
    "$$\n",
    "J = \\text{diag}(\\sigma(\\mathbf{z})) - \\sigma(\\mathbf{z}) \\sigma(\\mathbf{z})^\\top\n",
    "$$\n",
    "\n",
    "where $\\text{diag}(\\sigma(\\mathbf{z}))$ is a diagonal matrix with the softmax outputs on the diagonal, and $\\sigma(\\mathbf{z}) \\sigma(\\mathbf{z})^\\top$ is the outer product of the softmax vector with itself.\n",
    "\n",
    "## Softmax Regression and Gradient Simplification\n",
    "\n",
    "In softmax regression (also known as multinomial logistic regression), the goal is to predict the probability distribution over $K$ classes. The model typically uses the softmax function combined with the cross-entropy loss.\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "Given a true distribution $\\mathbf{y}$ (often one-hot encoded) and the predicted distribution $\\hat{\\mathbf{y}} = \\sigma(\\mathbf{z})$, the cross-entropy loss $L$ is:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^K y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "### Gradient of the Loss\n",
    "\n",
    "When computing the gradient of the loss with respect to the input $\\mathbf{z}$, the combination of the softmax function and cross-entropy loss leads to a significant simplification.\n",
    "\n",
    "#### Derivation\n",
    "\n",
    "1. **Compute $\\frac{\\partial L}{\\partial \\hat{y}_i}$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\frac{y_i}{\\hat{y}_i}\n",
    "$$\n",
    "\n",
    "2. **Compute $\\frac{\\partial \\hat{y}_i}{\\partial z_j}$ using the softmax gradient:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_j} = \\hat{y}_i (\\delta_{ij} - \\hat{y}_j)\n",
    "$$\n",
    "\n",
    "3. **Apply the chain rule to find $\\frac{\\partial L}{\\partial z_j}$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_j} = \\sum_{i=1}^K \\left(-\\frac{y_i}{\\hat{y}_i}\\right) \\hat{y}_i (\\delta_{ij} - \\hat{y}_j) = -y_j + \\hat{y}_j \\sum_{i=1}^K y_i = \\hat{y}_j - y_j\n",
    "$$\n",
    "\n",
    "### Simplified Gradient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = \\hat{y}_j - y_j\n",
    "$$\n",
    "\n",
    "### Implications\n",
    "\n",
    "- **No Need to Explicitly Compute the Softmax Gradient:** The gradient of the loss with respect to $\\mathbf{z}$ simplifies to $\\hat{\\mathbf{y}} - \\mathbf{y}$. This elegant result arises because the derivative of the cross-entropy loss cancels out the complexity of the softmax gradient.\n",
    "\n",
    "- **Efficiency in Computation:** This simplification allows for efficient gradient computation during training, avoiding the need to handle the full Jacobian matrix of the softmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each row of the input array.\n",
    "\n",
    "    Args:\n",
    "        z (np.ndarray): Input array of shape (n_samples, n_classes).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Softmax probabilities of shape (n_samples, n_classes).\n",
    "    \n",
    "    e^z / sum(e^z)\n",
    "    \"\"\"\n",
    "    # For numerical stability, subtract the max from each row\n",
    "    z_shift = z - np.max(z, axis=1, keepdims=True)\n",
    "    # apply softmax for each row\n",
    "    exp_z = np.exp(z_shift)\n",
    "    sum_exp_z = np.sum(exp_z, axis=1, keepdims=True)\n",
    "    return exp_z / sum_exp_z\n",
    "\n",
    "\n",
    "class CCE_Loss:\n",
    "    \"\"\"\n",
    "    Categorical Cross Entropy Loss\n",
    "    loss = -sum(y_true * log(y_pred))\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # To prevent log(0), add a small epsilon\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(y_true * np.log(y_pred), axis=1)  # Sum over classes\n",
    "        self.y_true, self.y_pred = y_true, y_pred\n",
    "        return np.mean(loss)  # Average over samples\n",
    "\n",
    "    def get_loss_grad(self):\n",
    "        \"\"\"\n",
    "        Gradient of CCE loss with respect to logits z\n",
    "        Assuming y_pred = softmax(z), the gradient is (y_pred - y_true) / n_samples\n",
    "        \"\"\"\n",
    "        loss_grad = (self.y_pred - self.y_true) / self.y_true.shape[0]\n",
    "        return loss_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxModel:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize weights and bias for multiclass classification.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            output_dim (int): Number of classes.\n",
    "        \"\"\"\n",
    "        self.weight = np.random.randn(input_dim, output_dim) * 0.01  # Small random weights\n",
    "        self.bias = np.zeros((1, output_dim))  # Bias initialized to zeros\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass: compute logits and softmax probabilities.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted probabilities of shape (n_samples, output_dim).\n",
    "        \"\"\"\n",
    "        self.X = X  # Store input for backward pass\n",
    "        z = X @ self.weight + self.bias  # Compute logits\n",
    "        y_pred = softmax(z)  # Apply softmax activation\n",
    "        return y_pred\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients of weights and bias.\n",
    "\n",
    "        Args:\n",
    "            loss_grad (np.ndarray): Gradient of loss w.r. to logits z, shape (n_samples, output_dim).\n",
    "        \"\"\"\n",
    "        self.weight_grad = self.X.T @ loss_grad  # Gradient w.r. to weights\n",
    "        self.bias_grad = np.sum(loss_grad, axis=0, keepdims=True)  # Gradient w.r. to bias\n",
    "\n",
    "    def step(self, lr=0.01):\n",
    "        \"\"\"\n",
    "        Update weights and bias using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate.\n",
    "        \"\"\"\n",
    "        self.weight -= lr * self.weight_grad\n",
    "        self.bias -= lr * self.bias_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource\n",
    "- https://awjuliani.medium.com/simple-softmax-in-python-tutorial-d6b4c4ed5c16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiclass_data(n_samples=300, n_features=2, n_classes=3):\n",
    "    \"\"\"\n",
    "    Generate synthetic multiclass classification data.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): Total number of samples.\n",
    "        n_features (int): Number of input features.\n",
    "        n_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing:\n",
    "            - X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "            - y_true (np.ndarray): One-hot encoded labels of shape (n_samples, n_classes).\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    X = np.random.randn(n_samples, n_features) # (n, p)\n",
    "    true_weights = np.random.randn(n_features, n_classes) # (n, k)\n",
    "    true_bias = np.random.randn(1, n_classes) # (1, 3) not (n, k) b.c hat each class has a single bias term that applies to all samples.\n",
    "\n",
    "    logits = X @ true_weights + true_bias\n",
    "    y_prob = softmax(logits)\n",
    "    y_indices = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    y_true = np.zeros((n_samples, n_classes)) ()\n",
    "    y_true[np.arange(n_samples), y_indices] = 1\n",
    "\n",
    "    return X, y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
