{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Position Information\n",
    "\n",
    "\n",
    "## Positional Embedding information has to be unique \n",
    "\n",
    "We need to assign position information to each token eg. pos_emb(\"i\") will be $[0, 0, 0]$\n",
    "\n",
    "```python\n",
    "i need help, however ..... i dont know,  i need  help\n",
    "|   |   |     |            |    |    |   |    |    |     \n",
    "^   ^   ^     ^            ^    ^    ^   ^    ^    ^\n",
    "0   1   2     3            25   26   27  28   29   30\n",
    "0   1   2     3            25   26   27  28   29   30\n",
    "0   1   2     3            25   26   27  28   29   30\n",
    "V   V   V     V            V     V   V   V     V   V\n",
    "```\n",
    "\n",
    "Above is the simplest position information where each number can be assigned with a int value and then we can add it to token embedding eg. dim = 3\n",
    "\n",
    "```python\n",
    "total_emb_i = token_emb(\"i\") + pos_emb(\"i\")\n",
    "= [\"0.34\", \"0.1\", \"-1.9\"] + [\"0\", \"0\", \"0\"]\n",
    "\n",
    "total_emb_need = token_emb(\"need\") + pos_emb(\"need\")\n",
    "= [\"-0.04\", \"0.12\", \"-1.09\"] + [\"1\", \"1\", \"1\"]\n",
    "\n",
    "\n",
    "total_emb_know = token_emb(\"need\") + pos_emb(\"need\")\n",
    "= [\"-0.90\", \"0.08\", \"-0.09\"] + [\"27\", \"27\", \"27\"] = [\"26.1\", \"27.08\", \"27.09\"]\n",
    "```\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "This leads us to 1st requirement when we need to design the positional embedding which is\n",
    "\n",
    "- **Positional Embedding has to be unique for each token. This is easy to understand since we need to know this is the 1st token and that is the 2nd etc.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding should be able to take indefinite long input\n",
    "\n",
    "\n",
    "```python\n",
    "i need help, however ..... i dont know,  i need  help\n",
    "|   |   |     |            |    |    |   |    |    |     \n",
    "^   ^   ^     ^            ^    ^    ^   ^    ^    ^\n",
    "0   1   2     3            25   26   27  28   29   30\n",
    "0   1   2     3            25   26   27  28   29   30\n",
    "0   1   2     3            25   26   27  28   29   30\n",
    "V   V   V     V            V     V   V   V     V   V\n",
    "```\n",
    "\n",
    "\n",
    "For above input, we compute a fixed sized position embedding matrix D*L = (3, 30), where each vector is unique. However, if we have a longer input which sequence length is 31, then this is a issue. Therefore, in order for positional encoding to be able to take indefinite long input, we need PE to be a function of sequence length eg. i. So PE = $f(i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Positional Encoding should contain Absolute Position Information\n",
    "\n",
    "In addition to that, intuitively speaking we should let positional encoding contains information that \\\"however_3\\\" should be at the front of \"i_25\" and stay after \"i_0\". \n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "i need help, however ..... i dont know,  i need  help\n",
    "|   |   |     |            |    |    |   |    |    |     \n",
    "^   ^   ^     ^            ^    ^    ^   ^    ^    ^\n",
    "0   1   2     3            25   26   27  28   29   30\n",
    "0   1   2     3            25   26   27  28   29   30\n",
    "0   1   2     3            25   26   27  28   29   30\n",
    "```\n",
    "\n",
    "</br>\n",
    "\n",
    "Given the intuitive positional encoding schema, we can see it indeed has that requirement where we know token got assigned with $[0, 0, 0]$ should be in the front of token assigned with $[3, 3, 3]$ because $0 < 3$\n",
    "\n",
    "</br>\n",
    "\n",
    "This leads us to 2nd requirement when we need to design the positional embedding which is\n",
    "\n",
    "- **Positional Embedding should contain absolute positional information.**\n",
    "\n",
    "where absolute Positional Encoding assigns a unique position vector to each position in the sequence. For example, the first word gets position 1, the second word position 2, and so on. This encoding is added to the token embeddings to incorporate positional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to that, positional encoding should encode the relative word information which is the distance between 2 tokens, which can be calculated as the difference in their positions.\n",
    "\n",
    "</br>\n",
    "\n",
    "```python\n",
    "I need help, however ..... I don't know, I need help\n",
    "```\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "eg. Let's focus on how the token \"help\" at position 3 attends to other tokens using relative positional encoding.\n",
    "\n",
    "Calculating Relative Positions:\n",
    "For the token at position 3 (\"help\"), we calculate the relative positions to all other tokens:\n",
    "\n",
    "| Token   | Absolute Position | Relative Position (Other Position - 3) |\n",
    "|---------|-------------------|----------------------------------------|\n",
    "| I       | 1                 | -2                                     |\n",
    "| need    | 2                 | -1                                     |\n",
    "| help,   | 3                 | 0                                      |\n",
    "| ,       | 4                 | +1                                     |\n",
    "| however | 5                 | +2                                     |\n",
    "| .....   | 6                 | +3                                     |\n",
    "| I       | 7                 | +4                                     |\n",
    "| don't   | 8                 | +5                                     |\n",
    "| know    | 9                 | +6                                     |\n",
    "| ,       | 10                | +7                                     |\n",
    "| I       | 11                | +8                                     |\n",
    "| need    | 12                | +9                                     |\n",
    "| help    | 13                | +10                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Information should contain relative position information\n",
    "\n",
    "Relative position information in positional encoding means the model considers the distances between tokens when processing the sentence. \n",
    "\n",
    "</br>\n",
    "\n",
    "```python\n",
    "I need help, however ..... I don't know, I need help\n",
    "```\n",
    "\n",
    "</br>\n",
    "\n",
    "For example, when the model looks at the word \"need\", it knows that \"help\" is one position away, both before and after the comma. This helps the model understand that \"need\" is closely related to \"help\" due to their consistent relative positions, even though they appear at different absolute positions in the sentence. By focusing on these relative distances, the model better captures the repeated pattern \"I need help\" and understands the relationships between words based on how far apart they are from each other.\n",
    "\n",
    "\n",
    "Relative Positional Encoding: Focuses on the relative distances between tokens. Instead of knowing that a word is at position 5, the model knows that one word is, say, three positions away from another. This can capture patterns like \"the next word,\" \"the previous word,\" or \"words within a certain range.\"\n",
    "\n",
    "\n",
    "Also, this proximity & closeness information can be used to adjust attention score from \"help\" (position 3) to \"need\" (position 2):\n",
    "\n",
    "Relative Position: $2−3=−1$\n",
    "The model uses the embedding for relative position -1 to adjust the attention score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However, we can say there're a lot of obvious limitations on above encoding idea\n",
    "\n",
    "if the sequency length is very long, our total_emb will be very heavy tailed. As you can see from total_emb_know, $[26.1, 27.08, 27.09]$ become overly large as the sequence length increases which intuitively speaking not a good thing. The embedding value for each dim for the last token will be super large (heavy tailed) compared with total_emb_i = $[0.34, 0.1, -1.9]$. Semantic meaning will be distorted by large position value.\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "```python\n",
    "i need help\n",
    "^   ^   ^ \n",
    "0   1   2 \n",
    "0   1   2 \n",
    "0   1   2 \n",
    "V   V   V \n",
    "\n",
    "\n",
    "i need help\n",
    "^   ^   ^  \n",
    "28  29  30  \n",
    "28  29  30  \n",
    "28  29  30  \n",
    "V   V   V  \n",
    "```\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "Another thing on heavy tail is that by using the encoding idea above, the final embedding for tokens \"i\", \"need\", \"help\" on position $29, 29, 30$ will be way different from the tokens \"i\", \"need\", \"help\" on position $0, 1, 2$ which will confused the model even if they should share some similar information (small distance on vector space between those two vectors.) Again, this is because since the position information overtaken the semantic embedding.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This leads us to the other 2 requirements when we need to design the positional embedding which is:\n",
    "- **Its better positional embedding could be upper & lower bounded by some value no matter how long the sequence length gonna be.**\n",
    "- **Its better same phrases on the different position should share some similarities on the vector space.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "- Positional Embedding information has to be unique.\n",
    "- Positional Encoding should be able to take indefinite long input.\n",
    "- Positional Encoding should contain Absolute Position Information. (index info)\n",
    "- Position Information should contain relative position information. (closeness info)\n",
    "- Its better positional embedding could be upper & lower bounded by some value no matter how long the sequence length gonna be.\n",
    "- Its better same phrases on the different position should share some similarities on the vector space.\n",
    "\n",
    "\n",
    "\n",
    "Position Embedding in Transformer:\n",
    "- https://www.youtube.com/watch?v=5V9gZcAd6cE&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For above positional encoding, what they are doing is adding positional embedding to token embedding and we do self-attention interaction between each token within the sequence.\n",
    "\n",
    "But how about we adding this positional encoding information onto self-attention mechanism matrix directly? Intuition behind this i think is obvious eg. the word far away from each other, may be we can gauge the self-attention score down a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
