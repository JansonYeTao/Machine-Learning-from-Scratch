{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Let the model be defined as:} \\quad z_i = \\mathbf{w}_i^\\top \\mathbf{x} + b_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Softmax function:} \\quad \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Loss function (Cross-Entropy Loss):} \\quad L = -\\sum_{i=1}^{K} y_i \\log(\\text{Softmax}(z_i))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Gradient of the loss with respect to } z_i:\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\text{Softmax}(z_i) - y_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Gradient with respect to the weight } \\mathbf{w}_i:\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}_i} = \\frac{\\partial L}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial \\mathbf{w}_i} = (\\text{Softmax}(z_i) - y_i) \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Gradient with respect to the bias } b_i:\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_i} = \\frac{\\partial L}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial b_i} = \\text{Softmax}(z_i) - y_i\n",
    "$$\n",
    "\n",
    "Summary of Gradients\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}_i} &= (\\text{Softmax}(z_i) - y_i) \\mathbf{x} \\\\\n",
    "\\frac{\\partial L}{\\partial b_i} &= \\text{Softmax}(z_i) - y_i\n",
    "\\end{aligned}\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Softmax}(z_i)}{\\partial z_j} =\n",
    "\\begin{cases}\n",
    "\\text{Softmax}(z_i) \\left(1 - \\text{Softmax}(z_i)\\right) & \\text{if } i = j \\\\\n",
    "- \\text{Softmax}(z_i) \\text{Softmax}(z_j) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{K} y_i \\log(\\text{Softmax}(z_i))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each row of the input array.\n",
    "\n",
    "    Args:\n",
    "        z (np.ndarray): Input array of shape (n_samples, n_classes).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Softmax probabilities of shape (n_samples, n_classes).\n",
    "    \n",
    "    e^z / sum(e^z)\n",
    "    \"\"\"\n",
    "    # For numerical stability, subtract the max from each row\n",
    "    z_shift = z - np.max(z, axis=1, keepdims=True)\n",
    "    # apply softmax for each row\n",
    "    exp_z = np.exp(z_shift)\n",
    "    sum_exp_z = np.sum(exp_z, axis=1, keepdims=True)\n",
    "    return exp_z / sum_exp_z\n",
    "\n",
    "\n",
    "class CCE_Loss:\n",
    "    \"\"\"\n",
    "    Categorical Cross Entropy Loss\n",
    "    loss = -sum(y_true * log(y_pred))\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # To prevent log(0), add a small epsilon\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(y_true * np.log(y_pred), axis=1)  # Sum over classes\n",
    "        self.y_true, self.y_pred = y_true, y_pred\n",
    "        return np.mean(loss)  # Average over samples\n",
    "\n",
    "    def get_loss_grad(self):\n",
    "        \"\"\"\n",
    "        Gradient of CCE loss with respect to logits z\n",
    "        Assuming y_pred = softmax(z), the gradient is (y_pred - y_true) / n_samples\n",
    "        \"\"\"\n",
    "        loss_grad = (self.y_pred - self.y_true) / self.y_true.shape[0]\n",
    "        return loss_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxModel:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize weights and bias for multiclass classification.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            output_dim (int): Number of classes.\n",
    "        \"\"\"\n",
    "        self.weight = np.random.randn(input_dim, output_dim) * 0.01  # Small random weights\n",
    "        self.bias = np.zeros((1, output_dim))  # Bias initialized to zeros\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass: compute logits and softmax probabilities.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted probabilities of shape (n_samples, output_dim).\n",
    "        \"\"\"\n",
    "        self.X = X  # Store input for backward pass\n",
    "        z = X @ self.weight + self.bias  # Compute logits\n",
    "        y_pred = softmax(z)  # Apply softmax activation\n",
    "        return y_pred\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients of weights and bias.\n",
    "\n",
    "        Args:\n",
    "            loss_grad (np.ndarray): Gradient of loss w.r. to logits z, shape (n_samples, output_dim).\n",
    "        \"\"\"\n",
    "        self.weight_grad = self.X.T @ loss_grad  # Gradient w.r. to weights\n",
    "        self.bias_grad = np.sum(loss_grad, axis=0, keepdims=True)  # Gradient w.r. to bias\n",
    "\n",
    "    def step(self, lr=0.01):\n",
    "        \"\"\"\n",
    "        Update weights and bias using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate.\n",
    "        \"\"\"\n",
    "        self.weight -= lr * self.weight_grad\n",
    "        self.bias -= lr * self.bias_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource\n",
    "- https://awjuliani.medium.com/simple-softmax-in-python-tutorial-d6b4c4ed5c16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiclass_data(n_samples=300, n_features=2, n_classes=3):\n",
    "    \"\"\"\n",
    "    Generate synthetic multiclass classification data.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): Total number of samples.\n",
    "        n_features (int): Number of input features.\n",
    "        n_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing:\n",
    "            - X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "            - y_true (np.ndarray): One-hot encoded labels of shape (n_samples, n_classes).\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    X = np.random.randn(n_samples, n_features) # (n, p)\n",
    "    true_weights = np.random.randn(n_features, n_classes) # (n, k)\n",
    "    true_bias = np.random.randn(1, n_classes) # (1, 3) not (n, k) b.c hat each class has a single bias term that applies to all samples.\n",
    "\n",
    "    logits = X @ true_weights + true_bias\n",
    "    y_prob = softmax(logits)\n",
    "    y_indices = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    y_true = np.zeros((n_samples, n_classes)) ()\n",
    "    y_true[np.arange(n_samples), y_indices] = 1\n",
    "\n",
    "    return X, y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
