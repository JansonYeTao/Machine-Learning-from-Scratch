{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss, Softmax and Overfitting\n",
    "\n",
    "Label smoothing is a way of adding noise at the output targets, aka labels. Letâ€™s assume that we have a classification problem. In most of them, we use a form of cross-entropy loss such as \n",
    "\n",
    "$$\n",
    "\\mathcal{L}_i = - \\sum_{j=1}^k y_{ij} \\log \\hat{y}_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "and softmax to output the final probabilities.\n",
    "\n",
    "\n",
    "The target vector has the form of $[0, 1 , 0 , 0]$. Because of the way softmax is formulated: \n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i / T}}{\\sum_{j} e^{z_j / T}}\n",
    "$$\n",
    "\n",
    "\n",
    "it can never achieve an output of 1 or 0. The best he can do is something like $[0.0003, 0.999, 0.0003, 0.0003]$. As a result, the model will continue to be trained, pushing the output values as high and as low as possible. The model will never converge. That, of course, will cause overfitting.\n",
    "\n",
    "To address that, label smoothing replaces the hard 0 and 1 targets by a small margi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
