{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error (MSE) Loss with L1 and L2 Regularization\n",
    "\n",
    "The MSE loss with L1 and L2 regularization is given by:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda_1 \\sum_{j} \\left| w_j \\right| + \\frac{\\lambda_2}{2} \\sum_{j} w_j^2\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $m$ is the number of data points.\n",
    "- $y_i$ is the true label, and $\\hat{y}_i$ is the prediction.\n",
    "- $\\lambda_1$ and $\\lambda_2$ are the L1 and L2 regularization coefficients, respectively.\n",
    "\n",
    "# Gradient of Loss with Respect to Weights and Bias\n",
    "\n",
    "To compute the gradients step by step:\n",
    "\n",
    "## 1. Compute the gradient of the MSE loss without regularization\n",
    "\n",
    "Let $y_i = w^T x_i + b$ be the predicted value. The gradient of the loss with respect to $w$ and $b$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{MSE}}}{\\partial w} = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right) x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{MSE}}}{\\partial b} = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right)\n",
    "$$\n",
    "\n",
    "## 2. Add the L1 regularization gradient\n",
    "\n",
    "The gradient of the L1 regularization term with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\lambda_1 \\sum_{j} \\left| w_j \\right|}{\\partial w_j} = \\lambda_1 \\cdot \\text{sign}(w_j)\n",
    "$$\n",
    "\n",
    "## 3. Add the L2 regularization gradient\n",
    "\n",
    "The gradient of the L2 regularization term with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\frac{\\lambda_2}{2} \\sum_{j} w_j^2}{\\partial w_j} = \\lambda_2 w_j\n",
    "$$\n",
    "\n",
    "## 4. Combine the gradients\n",
    "\n",
    "The total gradient with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right) x_i + \\lambda_1 \\cdot \\text{sign}(w) + \\lambda_2 w\n",
    "$$\n",
    "\n",
    "The total gradient with respect to $b$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LassoLinearModel: \n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # n, p: (row of data, feature#)\n",
    "        # project matrix input_dim -> output_dim (p, 1)\n",
    "        # we initialize weight by sampling from normal distribution since\n",
    "        # Max Likelihood + normal distribution = linear regression: \n",
    "        self.weight = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
    "        self.bias = np.zeros(output_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        # 2/n * (y - y_pred) * X = c * X\n",
    "        # 2/n * (y - y_pred) * 1 = c * 1\n",
    "        self.weight_grad = self.x.T @ loss_grad # (100, 1) @ (100, 1) = self.x @ loss_grad\n",
    "        self.bias_grad = loss_grad.sum(axis=0) # axis=1 right and axis=0 down\n",
    "\n",
    "    def step(self, lr):\n",
    "        # key part\n",
    "        self.weight = self.weight - lr * self.weight_grad\n",
    "        self.bias = self.bias - lr * self.bias_grad\n",
    "    \n",
    "\n",
    "class Loss: \n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        self.y_true, self.y_pred = y_true, y_pred\n",
    "        mse = np.mean((y_true - y_pred)**2) # mse\n",
    "        return mse\n",
    "\n",
    "    def get_loss_grad(self):\n",
    "        # key part\n",
    "        \"\"\"1/n * sum(   (y-pred)**2    ) where y_pred = Wx + b\n",
    "        dl/dw = dl/dy_pred * dy_pred/dw = 2/n * (y - y_pred) * X = c * X\n",
    "        dl/db = dl/dy_pred * dy_pred/db = 2/n * (y - y_pred) * 1 = c * 1\n",
    "        here we just return 2/n * (y - y_pred), part and interaction with X\n",
    "        part will be inside the LinearModel.backward()\n",
    "        \"\"\"\n",
    "        n = self.y_true.shape[0]\n",
    "        # remove n here see what will happen. answer: gradient exploding since too large\n",
    "        # this term is for averaging the gradient for data in batch\n",
    "        # otherwise it will overshoot the local minimal point. if n removed, you need to\n",
    "        # increasing learning rate on the other side.\n",
    "        return 2 / n * (self.y_pred - self.y_true) # c = - 2/n * (self.y_true - self.y_pred)\n",
    "\n",
    "    \n",
    "def one_epoch_train(i, X, y_true, model, loss):\n",
    "    y_pred = model(X)\n",
    "    loss_val = loss(y_true, y_pred)\n",
    "    loss_grad = loss.get_loss_grad()\n",
    "    print(f\"loss: {loss_val}\")\n",
    "    if i % 10 == 0:\n",
    "        plt.plot(X, y_pred)\n",
    "\n",
    "    model.backward(loss_grad) # update total gradient\n",
    "    model.step(0.02) # update parameters\n",
    "\n",
    "\n",
    "def train(X, y_true, model, loss, n_epoch):\n",
    "    for i in range(n_epoch):\n",
    "        one_epoch_train(i, X, y_true, model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
