{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition on Why L1 and L2 regularization can reduce overfitting?\n",
    "- extreme coefficients/weight are unlikely to yield good generalization.\n",
    "- Introducing a penalty to the sum of the weights means that the model has to “distribute” its weights optimally, so naturally most of this “resource” will go to the simple features that explain most of the variance, with complex features getting small or zero weights.\n",
    "- L1 useful for variable and feature selection as beta/weight got to zero\n",
    "- L2 much less likely to get zero coeffcients\n",
    "\n",
    "Normalization\n",
    "- We did not normalize our variable for the synthetic data set because we only had one variable. Normally, we have to convert all X variables to standard scores so they are all in the same range and zero centered. If the variables are all in different ranges, regularization will squash some coefficients more than the others because all regularization does is constrain coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error (MSE) Loss with L1 and L2 Regularization\n",
    "\n",
    "The MSE loss with L1 and L2 regularization is given by:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda_1 \\sum_{j} \\left| w_j \\right| + \\frac{\\lambda_2}{2} \\sum_{j} w_j^2\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $m$ is the number of data points.\n",
    "- $y_i$ is the true label, and $\\hat{y}_i$ is the prediction.\n",
    "- $\\lambda_1$ and $\\lambda_2$ are the L1 and L2 regularization coefficients, respectively.\n",
    "\n",
    "# Gradient of Loss with Respect to Weights and Bias\n",
    "\n",
    "To compute the gradients step by step:\n",
    "\n",
    "## 1. Compute the gradient of the MSE loss without regularization\n",
    "\n",
    "Let $y_i = w^T x_i + b$ be the predicted value. The gradient of the loss with respect to $w$ and $b$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{MSE}}}{\\partial w} = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right) x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{MSE}}}{\\partial b} = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right)\n",
    "$$\n",
    "\n",
    "## 2. Add the L1 regularization gradient\n",
    "\n",
    "The gradient of the L1 regularization term with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\lambda_1 \\sum_{j} \\left| w_j \\right|}{\\partial w_j} = \\lambda_1 \\cdot \\text{sign}(w_j)\n",
    "$$\n",
    "\n",
    "## 3. Add the L2 regularization gradient\n",
    "\n",
    "The gradient of the L2 regularization term with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\frac{\\lambda_2}{2} \\sum_{j} w_j^2}{\\partial w_j} = \\lambda_2 w_j\n",
    "$$\n",
    "\n",
    "## 4. Combine the gradients\n",
    "\n",
    "The total gradient with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right) x_i + \\lambda_1 \\cdot \\text{sign}(w) + \\lambda_2 w\n",
    "$$\n",
    "\n",
    "The total gradient with respect to $b$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i - \\hat{y}_i \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearModel: \n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # n, p: (row of data, feature#)\n",
    "        # project matrix input_dim -> output_dim (p, 1)\n",
    "        # we initialize weight by sampling from normal distribution since\n",
    "        # Max Likelihood + normal distribution = linear regression: \n",
    "        self.weight = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
    "        self.bias = np.zeros(output_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        # 2/n * (y - y_pred) * X = c * X\n",
    "        # 2/n * (y - y_pred) * 1 = c * 1\n",
    "        self.weight_grad = self.x.T @ loss_grad # (100, 1) @ (100, 1) = self.x @ loss_grad\n",
    "        self.bias_grad = loss_grad.sum(axis=0) # axis=1 right and axis=0 down\n",
    "\n",
    "        if self.l1_lambda is not None:\n",
    "            self.weight_grad += np.sign(self.weight)\n",
    "    \n",
    "        if self.l2_lambda is not None:\n",
    "            self.weight_grad += self.l2_lambda * self.weight\n",
    "\n",
    "    def step(self, lr):\n",
    "        # key part\n",
    "        self.weight = self.weight - lr * self.weight_grad\n",
    "        self.bias = self.bias - lr * self.bias_grad\n",
    "    \n",
    "\n",
    "class Loss: \n",
    "\n",
    "    def __init__(self, l1_lambda, l2_lambda, model):\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        self.y_true, self.y_pred = y_true, y_pred\n",
    "        mse = np.mean((y_true - y_pred)**2) # mse\n",
    "        l1 = self.l1_lambda * np.sum( np.abs(self.model.weight) )\n",
    "        l2 = self.l2_lambda * np.sum( np.sum(self.model.weight**2) )\n",
    "        return mse + l1 + l2\n",
    "\n",
    "    def get_loss_grad(self):\n",
    "        # key part\n",
    "        \"\"\"1/n * sum(   (y-pred)**2    ) where y_pred = Wx + b\n",
    "        dl/dw = dl/dy_pred * dy_pred/dw = 2/n * (y - y_pred) * X = c * X\n",
    "        dl/db = dl/dy_pred * dy_pred/db = 2/n * (y - y_pred) * 1 = c * 1\n",
    "        here we just return 2/n * (y - y_pred), part and interaction with X\n",
    "        part will be inside the LinearModel.backward()\n",
    "        \"\"\"\n",
    "        n = self.y_true.shape[0]\n",
    "        # remove n here see what will happen. answer: gradient exploding since too large\n",
    "        # this term is for averaging the gradient for data in batch\n",
    "        # otherwise it will overshoot the local minimal point. if n removed, you need to\n",
    "        # increasing learning rate on the other side.\n",
    "        return 2 / n * (self.y_pred - self.y_true) # c = - 2/n * (self.y_true - self.y_pred)\n",
    "\n",
    "    \n",
    "def one_epoch_train(i, X, y_true, model, loss):\n",
    "    y_pred = model(X)\n",
    "    loss_val = loss(y_true, y_pred)\n",
    "    loss_grad = loss.get_loss_grad()\n",
    "    print(f\"loss: {loss_val}\")\n",
    "    if i % 10 == 0:\n",
    "        plt.plot(X, y_pred)\n",
    "\n",
    "    model.backward(loss_grad) # update total gradient\n",
    "    model.step(0.02) # update parameters\n",
    "\n",
    "\n",
    "def train(X, y_true, model, loss, n_epoch):\n",
    "    for i in range(n_epoch):\n",
    "        one_epoch_train(i, X, y_true, model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
