{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Intuition:\n",
    "\n",
    "Dropout is applied to the activations of the Hidden Layer during training. Preventing Co-Adaptation:\n",
    "\n",
    "- Co-adaptation occurs when neurons adjust their weights in a way that depends on the presence of other neurons. This can lead to overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "\n",
    "- By randomly dropping activations, dropout ensures that neurons cannot rely on specific other neurons being present, promoting independent feature learning.\n",
    "\n",
    "Suppose we have 4 neurons at one layer, A, B, C, D. With some random unexpected initialization, it is possible that the weights of A & B are closed to zero. Imagine the following two situations:\n",
    "\n",
    "If we don‚Äôt have dropout, this layer only relay on neurons C & D to transform information to the next layer. So the neurons A & B might be too ‚Äúlazy‚Äù to adjust its weight since there are C & D.\n",
    "\n",
    "If we set dropout = 0.5, then at each epoch, two neurons are dropped randomly. So it is possible that both C and D are dropped, so A & B have to transform information and adopt the gradients weights adjustment. (This make sure learned weight are evenly distributed to existed neuron)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Sequential(\n",
    "    (0): Flatten()\n",
    "    (1): Linear(in_features=784, out_features=256, bias=True)\n",
    "    (2): ReLU()\n",
    "    (3): Dropout(p=0.5, inplace=False) # ---> dropout after linear + activation\n",
    "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
    "    (5): ReLU()\n",
    "    (6): Dropout(p=0.2, inplace=False)\n",
    "    (7): Linear(in_features=256, out_features=10, bias=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "### During Training:\n",
    "- Dropout randomly \"drops out\" (i.e., sets to zero) a fraction of the neurons' activations in a layer. This is achieved by generating a binary mask where each neuron is kept with a probability ùëù\n",
    "(the dropout rate) and dropped with probability $1‚àíùëù$\n",
    "- Scaling: To maintain the expected value of the activations, the remaining (non-dropped) activations are scaled by $1/ùëù$. This ensures that the overall magnitude of the activations remains consistent between training and \n",
    "- in pytorch: model.train()\n",
    "\n",
    "\n",
    "### During Inference.\n",
    "- During Evaluation: Dropout is typically disabled, meaning all neurons are active, and no scaling is applied.\n",
    "- in pytorch model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "Dropout Behavior:\n",
    "\n",
    "Recall that for training model order -> Forward pass -> loss -> Backward -> get gradient ....\n",
    "\n",
    "Gradient Flow: The same dropout mask used in the forward pass is applied to the gradients during the backward pass. This means that gradients corresponding to the dropped neurons are also zeroed out, ensuring that these neurons do not contribute to weight updates. (ÂÅ∑ÊáíÁöÑ‰∫∫ w=> 0 Ë¢´ÊùÄÊéâ lay offÔºå‰∏çÂèÇ‰∏égradient update). The same mask used in forward pass will be applied. Only the neuron kept in forward pass will received the gradients update, neuron not kept will be frozen and no gradient update\n",
    "\n",
    "eg. if the first and last neurons of a layer are dropped out in the first forward pass, a gradient is eventually calculated at the output and then backpropagated. When the second forward pass starts, a different set of neurons will be dropped out by temporarily frozing their weights.\n",
    "\n",
    "\n",
    "\n",
    "Scaling: Similar to the forward pass, the gradients are scaled by $1/ùëù$ to account for the scaling applied during the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dropout](https://d2l.ai/_images/dropout2.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, rate):\n",
    "    if not (0 <= rate <= 1):\n",
    "        raise NotImplemented(\"invalid rate value\")\n",
    "    \n",
    "    if rate == 1:\n",
    "        return torch.zeros_like(X)\n",
    "    \n",
    "    mask = (torch.rand(X.shape) > rate) # binary mask with T/F\n",
    "    results = X * mask # randomly zero out value inside X matrix\n",
    "    # scales up to make sure expectation of neuron is same\n",
    "    # X = [1, 1, 1, 1, 1] with 5 neurons, p = 0.8\n",
    "    # for each neuron expect(X[0]) = 1, 0.8 to keep and 0.2 to drop \n",
    "    # so cur expected value of X[0] = 1 * 0.8 + 0 * 0.2 = 0.8 instead of 1\n",
    "    # to correct this  X[0] / p = 0.8 / 0.8 = 1 so expectation back to 1\n",
    "    results = results / rate\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16).reshape((2, 8))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  2.,  4.,  0.,  8.,  0.,  0.,  0.],\n",
       "        [ 0.,  0., 20., 22.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dropout(X, 0.5)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout in pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output with Dropout: tensor([[-0.1167],\n",
      "        [-0.2020],\n",
      "        [-0.3047],\n",
      "        [-0.1872],\n",
      "        [-0.0012]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Training Loss: 1.78287672996521\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.l2 = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x = self.l1(X)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleModel()\n",
    "model.train() # <------ Turn on dropout\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_data = torch.randn(5, 10) # shape = (5, 10)\n",
    "target_data = torch.randn(5, 1) # shape = (5, 1)\n",
    "\n",
    "# Forward pass\n",
    "output_train = model(train_data)\n",
    "loss_train = criterion(output_train, target_data)\n",
    "\n",
    "# Backward pass and optimization\n",
    "optimizer.zero_grad()\n",
    "loss_train.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Training Output with Dropout: {output_train}\")\n",
    "print(f\"\\nTraining Loss: {loss_train.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Output without Dropout:\n",
      "tensor([[-0.0591],\n",
      "        [ 0.0543],\n",
      "        [-0.0431],\n",
      "        [-0.0533],\n",
      "        [ 0.0093]])\n",
      "\n",
      "Validation Loss:\n",
      "1.2603604793548584\n"
     ]
    }
   ],
   "source": [
    "# set model to evaluation mode for validation and test\n",
    "model.eval()\n",
    "\n",
    "# Example validation data\n",
    "input_val = torch.randn(5, 10)\n",
    "target_val = torch.randn(5, 1)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output_val = model(input_val)\n",
    "    loss_val = criterion(output_val, target_val)\n",
    "\n",
    "print(\"\\nValidation Output without Dropout:\")\n",
    "print(output_val)\n",
    "print(\"\\nValidation Loss:\")\n",
    "print(loss_val.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Dropout tech\n",
    "- DropConnect is a variant of the traditional dropout technique introduced by Wan et al. in their 2013 paper, \"Regularization of Neural Networks using DropConnect\". Unlike standard dropout, which randomly drops activations, DropConnect randomly drops weights during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
